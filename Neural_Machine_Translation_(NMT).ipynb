{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Neural Machine Translation (NMT)",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "63b4799a0c9740759d08c0859ed7c1b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_3c656ae874ca4adb82be5bc98649f7da",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_c03b5d46910f459d9fa31eee4d955b1f",
              "IPY_MODEL_f43c84589b7249fab22c3ca413ad7c33"
            ]
          }
        },
        "3c656ae874ca4adb82be5bc98649f7da": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c03b5d46910f459d9fa31eee4d955b1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_6581ab2c3b6c4c36ab5089abe25defb8",
            "_dom_classes": [],
            "description": "Dl Completed...: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9e9bc7fbc7a248ab9635a80528a86234"
          }
        },
        "f43c84589b7249fab22c3ca413ad7c33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_72a9fe1be4c14846bf8946c6e1b9a2a1",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1/1 [00:07&lt;00:00,  7.42s/ url]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6267e58877a4403ea9c44e31237ffacb"
          }
        },
        "6581ab2c3b6c4c36ab5089abe25defb8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9e9bc7fbc7a248ab9635a80528a86234": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "72a9fe1be4c14846bf8946c6e1b9a2a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6267e58877a4403ea9c44e31237ffacb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4a111f2df38548a09584328a18e92ee6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_46ee7e68707c4ea6af4634049392a221",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_e479b675e1ee42cd89bdb99e47ad190b",
              "IPY_MODEL_771a87fe3d7e443290f0766ffdf8b751"
            ]
          }
        },
        "46ee7e68707c4ea6af4634049392a221": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e479b675e1ee42cd89bdb99e47ad190b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_62a6c55e10e84c96bd85d4d66fbd9c1c",
            "_dom_classes": [],
            "description": "Dl Size...: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f0d929e814684149bbe93f86a5327336"
          }
        },
        "771a87fe3d7e443290f0766ffdf8b751": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d503f65898b14f989cd1b826bf06f1ee",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 34/34 [00:07&lt;00:00,  4.60 MiB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b7c3d7050b2441179397a2ffa8cdd613"
          }
        },
        "62a6c55e10e84c96bd85d4d66fbd9c1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f0d929e814684149bbe93f86a5327336": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d503f65898b14f989cd1b826bf06f1ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b7c3d7050b2441179397a2ffa8cdd613": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b90f031b92a046e1ad3f5b1631198b35": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_43bc6037ff414bb5b497aa539dd3d997",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_529d2062be024535bfb78b8e7c703a67",
              "IPY_MODEL_887745e553ac43a9bcefd96bcb7b8889"
            ]
          }
        },
        "43bc6037ff414bb5b497aa539dd3d997": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "529d2062be024535bfb78b8e7c703a67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_ba4a52bceed54ec5944e42540235cb36",
            "_dom_classes": [],
            "description": "Extraction completed...: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f269df658ae4411cb591105aa4e568c9"
          }
        },
        "887745e553ac43a9bcefd96bcb7b8889": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ea07a651ebe84688866edf919ef0a2f3",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1/1 [00:07&lt;00:00,  7.35s/ file]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_19ec179f02c0452bb1477563c197fe38"
          }
        },
        "ba4a52bceed54ec5944e42540235cb36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f269df658ae4411cb591105aa4e568c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ea07a651ebe84688866edf919ef0a2f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "19ec179f02c0452bb1477563c197fe38": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bd6049f33ac44700ac9b29f05f2ed5ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_018dcdff83af4f34be1e9b3799032e45",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_869073aacb2a4060942004c62bb38b64",
              "IPY_MODEL_ccb50b4be6b0488089d784188b8e427a"
            ]
          }
        },
        "018dcdff83af4f34be1e9b3799032e45": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "869073aacb2a4060942004c62bb38b64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_9d126a3742cb4aab91283c5b95c3b639",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ac2d0c74b76a4257a0a3ea4765725df8"
          }
        },
        "ccb50b4be6b0488089d784188b8e427a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_11bf571d85d246969cb36bd826a91479",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1108752/0 [03:01&lt;00:00, 5965.19 examples/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_efa7fa4a176b45dab4c3ab9ac81d9ce5"
          }
        },
        "9d126a3742cb4aab91283c5b95c3b639": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ac2d0c74b76a4257a0a3ea4765725df8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "11bf571d85d246969cb36bd826a91479": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "efa7fa4a176b45dab4c3ab9ac81d9ce5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "93a6409603af4eb38795eb90d87aa952": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_78f05fea90794afa978b58d180a5c4c2",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_f6c914637a644e65928cf82d0711ef9c",
              "IPY_MODEL_c3d6408c2fa8451085a84ec9e0e4754f"
            ]
          }
        },
        "78f05fea90794afa978b58d180a5c4c2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f6c914637a644e65928cf82d0711ef9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_1f87d5b9057e4ce5950b7474b4ad6bb6",
            "_dom_classes": [],
            "description": " 98%",
            "_model_name": "FloatProgressModel",
            "bar_style": "danger",
            "max": 1108752,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1086146,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ce6fe15bbcdb4d8d99f5e10d23792da5"
          }
        },
        "c3d6408c2fa8451085a84ec9e0e4754f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b3f972ff1a0744a5affc36d4a45ac101",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1086146/1108752 [00:04&lt;00:15, 1486.02 examples/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_fd18b44d770646a691c2eaabe3595b44"
          }
        },
        "1f87d5b9057e4ce5950b7474b4ad6bb6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ce6fe15bbcdb4d8d99f5e10d23792da5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b3f972ff1a0744a5affc36d4a45ac101": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "fd18b44d770646a691c2eaabe3595b44": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQNh-BB-AaZw"
      },
      "source": [
        "# **Neural Machine Translation using LSTM Seq2Seq Model with Scale Dot Product Attention**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M42CWGoBcUsm",
        "outputId": "3c30650a-be37-46cf-99b7-ac6920955e5f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install trax"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting trax\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/04/0c04116bbb372f459ad0a73bf306c5000f9fd63a8419bb179381f54773aa/trax-1.3.5-py2.py3-none-any.whl (416kB)\n",
            "\r\u001b[K     |▉                               | 10kB 25.1MB/s eta 0:00:01\r\u001b[K     |█▋                              | 20kB 2.9MB/s eta 0:00:01\r\u001b[K     |██▍                             | 30kB 3.6MB/s eta 0:00:01\r\u001b[K     |███▏                            | 40kB 4.1MB/s eta 0:00:01\r\u001b[K     |████                            | 51kB 3.4MB/s eta 0:00:01\r\u001b[K     |████▊                           | 61kB 3.7MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 71kB 4.2MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 81kB 4.4MB/s eta 0:00:01\r\u001b[K     |███████                         | 92kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 102kB 4.6MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 112kB 4.6MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 122kB 4.6MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 133kB 4.6MB/s eta 0:00:01\r\u001b[K     |███████████                     | 143kB 4.6MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 153kB 4.6MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 163kB 4.6MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 174kB 4.6MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 184kB 4.6MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 194kB 4.6MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 204kB 4.6MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 215kB 4.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 225kB 4.6MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 235kB 4.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 245kB 4.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 256kB 4.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 266kB 4.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 276kB 4.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 286kB 4.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 296kB 4.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 307kB 4.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 317kB 4.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 327kB 4.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 337kB 4.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 348kB 4.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 358kB 4.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 368kB 4.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 378kB 4.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 389kB 4.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 399kB 4.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 409kB 4.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 419kB 4.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: gin-config in /usr/local/lib/python3.6/dist-packages (from trax) (0.3.0)\n",
            "Collecting t5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/a4/1188812f1e439bba23f7ceb03879bcdb99744e2e12799aebb93bc0139496/t5-0.7.0-py3-none-any.whl (171kB)\n",
            "\u001b[K     |████████████████████████████████| 174kB 12.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: jax in /usr/local/lib/python3.6/dist-packages (from trax) (0.2.0)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.6/dist-packages (from trax) (0.1.55)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from trax) (1.18.5)\n",
            "Collecting tensor2tensor\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d6/7c/9e87d30cefad5cbc390bb7f626efb3ded9b19416b8160f1a1278da81b218/tensor2tensor-1.15.7-py2.py3-none-any.whl (1.4MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 14.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (from trax) (0.17.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from trax) (1.4.1)\n",
            "Collecting funcsigs\n",
            "  Downloading https://files.pythonhosted.org/packages/69/cb/f5be453359271714c01b9bd06126eaf2e368f1fddfff30818754b5ac2328/funcsigs-1.0.2-py2.py3-none-any.whl\n",
            "Collecting tensorflow-text\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/28/b2/2dbd90b93913afd07e6101b8b84327c401c394e60141c1e98590038060b3/tensorflow_text-2.3.0-cp36-cp36m-manylinux1_x86_64.whl (2.6MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6MB 37.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.6/dist-packages (from trax) (0.10.0)\n",
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.6/dist-packages (from trax) (2.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from trax) (1.15.0)\n",
            "Collecting sacrebleu\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/c4/8e948f601a4f9609e8b2b58f31966cb13cf17b940b82aa3e767f01c42c52/sacrebleu-1.4.14-py3-none-any.whl (64kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 11.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from t5->trax) (3.2.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from t5->trax) (1.6.0+cu101)\n",
            "Collecting rouge-score\n",
            "  Downloading https://files.pythonhosted.org/packages/1f/56/a81022436c08b9405a5247b71635394d44fe7e1dbedc4b28c740e09c2840/rouge_score-0.0.4-py2.py3-none-any.whl\n",
            "Collecting transformers>=2.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/19/22/aff234f4a841f8999e68a7a94bdd4b60b4cebcfeca5d67d61cd08c9179de/transformers-3.3.1-py3-none-any.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 60.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: babel in /usr/local/lib/python3.6/dist-packages (from t5->trax) (2.8.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from t5->trax) (1.1.2)\n",
            "Collecting mesh-tensorflow[transformer]>=0.1.13\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/16/8b/553deb763ce8d00afb17debab7cb14a87b209cd4c6f0e8ecfc8d884cb12a/mesh_tensorflow-0.1.17-py3-none-any.whl (342kB)\n",
            "\u001b[K     |████████████████████████████████| 348kB 51.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from t5->trax) (0.22.2.post1)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 59.4MB/s \n",
            "\u001b[?25hCollecting tfds-nightly\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bd/0a/f76915feba37b2f0fad58dab2e2559cd6842e84f1cdc88696f35f15201e4/tfds_nightly-4.0.1.dev202010140107-py3-none-any.whl (3.5MB)\n",
            "\u001b[K     |████████████████████████████████| 3.6MB 57.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: opt-einsum in /usr/local/lib/python3.6/dist-packages (from jax->trax) (3.3.0)\n",
            "Collecting gunicorn\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/ca/926f7cd3a2014b16870086b2d0fdc84a9e49473c68a8dff8b57f7c156f43/gunicorn-20.0.4-py2.py3-none-any.whl (77kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 12.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from tensor2tensor->trax) (4.41.1)\n",
            "Collecting tensorflow-probability==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3e/3a/c10b6c22320531c774402ac7186d1b673374e2a9d12502cbc8d811e4601c/tensorflow_probability-0.7.0-py2.py3-none-any.whl (981kB)\n",
            "\u001b[K     |████████████████████████████████| 983kB 54.6MB/s \n",
            "\u001b[?25hCollecting tensorflow-gan\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0c/2e/62922111d7d50e1900e3030764743ea7735540ce103b3ab30fd5cd2d8a2b/tensorflow_gan-2.0.0-py2.py3-none-any.whl (365kB)\n",
            "\u001b[K     |████████████████████████████████| 368kB 51.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from tensor2tensor->trax) (7.0.0)\n",
            "Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.6/dist-packages (from tensor2tensor->trax) (0.8.3)\n",
            "Collecting tf-slim\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/97/b0f4a64df018ca018cc035d44f2ef08f91e2e8aa67271f6f19633a015ff7/tf_slim-1.1.0-py2.py3-none-any.whl (352kB)\n",
            "\u001b[K     |████████████████████████████████| 358kB 51.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from tensor2tensor->trax) (2.10.0)\n",
            "Collecting kfac\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1c/36/06fe2c757044bb51906fef231ac48cc5bf9a277fc9a8c7e1108d7e9e8cfd/kfac-0.2.3-py2.py3-none-any.whl (191kB)\n",
            "\u001b[K     |████████████████████████████████| 194kB 57.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from tensor2tensor->trax) (0.16.0)\n",
            "Collecting bz2file\n",
            "  Downloading https://files.pythonhosted.org/packages/61/39/122222b5e85cd41c391b68a99ee296584b2a2d1d233e7ee32b4532384f2d/bz2file-0.98.tar.gz\n",
            "Requirement already satisfied: oauth2client in /usr/local/lib/python3.6/dist-packages (from tensor2tensor->trax) (4.1.3)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from tensor2tensor->trax) (4.1.2.30)\n",
            "Collecting pypng\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/fb/f719f1ac965e2101aa6ea6f54ef8b40f8fbb033f6ad07c017663467f5147/pypng-0.0.20.tar.gz (649kB)\n",
            "\u001b[K     |████████████████████████████████| 655kB 48.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-api-python-client in /usr/local/lib/python3.6/dist-packages (from tensor2tensor->trax) (1.7.12)\n",
            "Collecting gevent\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3f/92/b80b922f08f222faca53c8d278e2e612192bc74b0e1f0db2f80a6ee46982/gevent-20.9.0-cp36-cp36m-manylinux2010_x86_64.whl (5.3MB)\n",
            "\u001b[K     |████████████████████████████████| 5.3MB 45.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from tensor2tensor->trax) (2.23.0)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.6/dist-packages (from tensor2tensor->trax) (1.1.2)\n",
            "Requirement already satisfied: dopamine-rl in /usr/local/lib/python3.6/dist-packages (from tensor2tensor->trax) (1.0.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.6/dist-packages (from tensor2tensor->trax) (1.1.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym->trax) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.4.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym->trax) (1.3.0)\n",
            "Requirement already satisfied: tensorflow<2.4,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-text->trax) (2.3.0)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->trax) (0.24.0)\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->trax) (20.2.0)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->trax) (2.3)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->trax) (1.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->trax) (1.12.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->trax) (3.12.4)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->trax) (0.3.2)\n",
            "Collecting portalocker\n",
            "  Downloading https://files.pythonhosted.org/packages/89/a6/3814b7107e0788040870e8825eebf214d72166adf656ba7d4bf14759a06a/portalocker-2.0.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers>=2.7.0->t5->trax) (20.4)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers>=2.7.0->t5->trax) (0.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers>=2.7.0->t5->trax) (3.0.12)\n",
            "Collecting tokenizers==0.8.1.rc2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/83/8b9fccb9e48eeb575ee19179e2bdde0ee9a1904f97de5f02d19016b8804f/tokenizers-0.8.1rc2-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 57.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers>=2.7.0->t5->trax) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 52.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz>=2015.7 in /usr/local/lib/python3.6/dist-packages (from babel->t5->trax) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->t5->trax) (2.8.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->t5->trax) (0.16.0)\n",
            "Collecting importlib-resources; python_version < \"3.9\"\n",
            "  Downloading https://files.pythonhosted.org/packages/ba/03/0f9595c0c2ef12590877f3c47e5f579759ce5caf817f8256d5dcbd8a1177/importlib_resources-3.0.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: setuptools>=3.0 in /usr/local/lib/python3.6/dist-packages (from gunicorn->tensor2tensor->trax) (50.3.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability==0.7.0->tensor2tensor->trax) (4.4.2)\n",
            "Requirement already satisfied: tensorflow-hub>=0.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gan->tensor2tensor->trax) (0.9.0)\n",
            "Requirement already satisfied: typeguard in /usr/local/lib/python3.6/dist-packages (from tensorflow-addons->tensor2tensor->trax) (2.7.1)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client->tensor2tensor->trax) (4.6)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client->tensor2tensor->trax) (0.2.8)\n",
            "Requirement already satisfied: httplib2>=0.9.1 in /usr/local/lib/python3.6/dist-packages (from oauth2client->tensor2tensor->trax) (0.17.4)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client->tensor2tensor->trax) (0.4.8)\n",
            "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client->tensor2tensor->trax) (1.17.2)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client->tensor2tensor->trax) (0.0.4)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client->tensor2tensor->trax) (3.0.1)\n",
            "Collecting zope.interface\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c5/89/1eb9dbb9e24f5e2c29ab1a88097b2f1333858aac3cd3cccc6c4c1c8ad867/zope.interface-5.1.2-cp36-cp36m-manylinux2010_x86_64.whl (236kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 56.7MB/s \n",
            "\u001b[?25hCollecting zope.event\n",
            "  Downloading https://files.pythonhosted.org/packages/9e/85/b45408c64f3b888976f1d5b37eed8d746b8d5729a66a49ec846fda27d371/zope.event-4.5.0-py2.py3-none-any.whl\n",
            "Collecting greenlet>=0.4.17; platform_python_implementation == \"CPython\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/d0/532e160c777b42f6f393f9de8c88abb8af6c892037c55e4d3a8a211324dd/greenlet-0.4.17-cp36-cp36m-manylinux1_x86_64.whl (44kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 9.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->tensor2tensor->trax) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->tensor2tensor->trax) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->tensor2tensor->trax) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->tensor2tensor->trax) (3.0.4)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from flask->tensor2tensor->trax) (7.1.2)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from flask->tensor2tensor->trax) (1.1.0)\n",
            "Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.6/dist-packages (from flask->tensor2tensor->trax) (1.0.1)\n",
            "Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.6/dist-packages (from flask->tensor2tensor->trax) (2.11.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.6/dist-packages (from sympy->tensor2tensor->trax) (1.1.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (0.3.3)\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (1.1.2)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (0.35.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (1.32.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (2.3.0)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (1.6.3)\n",
            "Requirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (2.3.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (0.2.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-metadata->tensorflow-datasets->trax) (1.52.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers>=2.7.0->t5->trax) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-resources; python_version < \"3.9\"->tfds-nightly->t5->trax) (3.2.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client->tensor2tensor->trax) (4.1.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.10.1->flask->tensor2tensor->trax) (1.1.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (3.2.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (1.7.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (0.4.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (2.0.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (1.3.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (3.1.0)\n",
            "Building wheels for collected packages: bz2file, pypng, sacremoses\n",
            "  Building wheel for bz2file (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bz2file: filename=bz2file-0.98-cp36-none-any.whl size=6884 sha256=19bb4fa77e5342e9fa11135c14095d7eaf9290020024b4e6bd3ac56602578e11\n",
            "  Stored in directory: /root/.cache/pip/wheels/81/75/d6/e1317bf09bf1af5a30befc2a007869fa6e1f516b8f7c591cb9\n",
            "  Building wheel for pypng (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypng: filename=pypng-0.0.20-cp36-none-any.whl size=67162 sha256=878cb709f55e3a9b0ba3a797855c79e6fdcf1290db327018be3eb74b18dc3fa7\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/6b/ef/0493b536b6d4722c2ae9486691b1d49b922b9877922beeabb3\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=8e89a84410eba1dcad43f50ca6a8300f931ddb6ce6792a2954460c3651a3db7e\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built bz2file pypng sacremoses\n",
            "\u001b[31mERROR: kfac 0.2.3 has requirement tensorflow-probability==0.8, but you'll have tensorflow-probability 0.7.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: portalocker, sacrebleu, tensorflow-text, rouge-score, sentencepiece, tokenizers, sacremoses, transformers, mesh-tensorflow, importlib-resources, tfds-nightly, t5, gunicorn, tensorflow-probability, tensorflow-gan, tf-slim, kfac, bz2file, pypng, zope.interface, zope.event, greenlet, gevent, tensor2tensor, funcsigs, trax\n",
            "  Found existing installation: tensorflow-probability 0.11.0\n",
            "    Uninstalling tensorflow-probability-0.11.0:\n",
            "      Successfully uninstalled tensorflow-probability-0.11.0\n",
            "Successfully installed bz2file-0.98 funcsigs-1.0.2 gevent-20.9.0 greenlet-0.4.17 gunicorn-20.0.4 importlib-resources-3.0.0 kfac-0.2.3 mesh-tensorflow-0.1.17 portalocker-2.0.0 pypng-0.0.20 rouge-score-0.0.4 sacrebleu-1.4.14 sacremoses-0.0.43 sentencepiece-0.1.91 t5-0.7.0 tensor2tensor-1.15.7 tensorflow-gan-2.0.0 tensorflow-probability-0.7.0 tensorflow-text-2.3.0 tf-slim-1.1.0 tfds-nightly-4.0.1.dev202010140107 tokenizers-0.8.1rc2 transformers-3.3.1 trax-1.3.5 zope.event-4.5.0 zope.interface-5.1.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMfrLMIsdGug",
        "outputId": "9286d9f5-d331-467f-9ff4-353c8486a94f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from termcolor import colored\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "import trax\n",
        "from trax import layers as tl\n",
        "from trax.fastmath import numpy as fastnp\n",
        "from trax.supervised import training\n",
        "\n",
        "!pip list | grep trax"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "trax                          1.3.5                \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPmACv_ydQjQ",
        "outputId": "c3185ca7-9c54-4e33-b3b5-632c170b63ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305,
          "referenced_widgets": [
            "63b4799a0c9740759d08c0859ed7c1b3",
            "3c656ae874ca4adb82be5bc98649f7da",
            "c03b5d46910f459d9fa31eee4d955b1f",
            "f43c84589b7249fab22c3ca413ad7c33",
            "6581ab2c3b6c4c36ab5089abe25defb8",
            "9e9bc7fbc7a248ab9635a80528a86234",
            "72a9fe1be4c14846bf8946c6e1b9a2a1",
            "6267e58877a4403ea9c44e31237ffacb",
            "4a111f2df38548a09584328a18e92ee6",
            "46ee7e68707c4ea6af4634049392a221",
            "e479b675e1ee42cd89bdb99e47ad190b",
            "771a87fe3d7e443290f0766ffdf8b751",
            "62a6c55e10e84c96bd85d4d66fbd9c1c",
            "f0d929e814684149bbe93f86a5327336",
            "d503f65898b14f989cd1b826bf06f1ee",
            "b7c3d7050b2441179397a2ffa8cdd613",
            "b90f031b92a046e1ad3f5b1631198b35",
            "43bc6037ff414bb5b497aa539dd3d997",
            "529d2062be024535bfb78b8e7c703a67",
            "887745e553ac43a9bcefd96bcb7b8889",
            "ba4a52bceed54ec5944e42540235cb36",
            "f269df658ae4411cb591105aa4e568c9",
            "ea07a651ebe84688866edf919ef0a2f3",
            "19ec179f02c0452bb1477563c197fe38",
            "bd6049f33ac44700ac9b29f05f2ed5ee",
            "018dcdff83af4f34be1e9b3799032e45",
            "869073aacb2a4060942004c62bb38b64",
            "ccb50b4be6b0488089d784188b8e427a",
            "9d126a3742cb4aab91283c5b95c3b639",
            "ac2d0c74b76a4257a0a3ea4765725df8",
            "11bf571d85d246969cb36bd826a91479",
            "efa7fa4a176b45dab4c3ab9ac81d9ce5",
            "93a6409603af4eb38795eb90d87aa952",
            "78f05fea90794afa978b58d180a5c4c2",
            "f6c914637a644e65928cf82d0711ef9c",
            "c3d6408c2fa8451085a84ec9e0e4754f",
            "1f87d5b9057e4ce5950b7474b4ad6bb6",
            "ce6fe15bbcdb4d8d99f5e10d23792da5",
            "b3f972ff1a0744a5affc36d4a45ac101",
            "fd18b44d770646a691c2eaabe3595b44"
          ]
        }
      },
      "source": [
        "#Get generator function for training set\n",
        "train_stream_fn = trax.data.TFDS('opus/medical', keys = ('en', 'de'), eval_holdout_size=0.01, train = True)\n",
        "\n",
        "#Get generator function for validation set\n",
        "eval_stream_fn = trax.data.TFDS('opus/medical', keys = ('en', 'de'), eval_holdout_size=0.01, train = False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1mDownloading and preparing dataset opus/medical/0.1.0 (download: 34.29 MiB, generated: 188.85 MiB, total: 223.13 MiB) to /root/tensorflow_datasets/opus/medical/0.1.0...\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "63b4799a0c9740759d08c0859ed7c1b3",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Dl Completed...', max=1.0, style=Progre…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4a111f2df38548a09584328a18e92ee6",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Dl Size...', max=1.0, style=ProgressSty…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b90f031b92a046e1ad3f5b1631198b35",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Extraction completed...', max=1.0, styl…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bd6049f33ac44700ac9b29f05f2ed5ee",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\rShuffling and writing examples to /root/tensorflow_datasets/opus/medical/0.1.0.incompleteGUBXX1/opus-train.tfrecord\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "93a6409603af4eb38795eb90d87aa952",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1108752.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[1mDataset opus downloaded and prepared to /root/tensorflow_datasets/opus/medical/0.1.0. Subsequent calls will reuse this data.\u001b[0m\n",
            "\r"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5v5BP9xpdQlz",
        "outputId": "5be5332a-8796-4268-9747-91553f1d5a06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "train_stream = train_stream_fn()\n",
        "print(colored('train data (en, de) tuple:', 'red'), next(train_stream))\n",
        "\n",
        "eval_stream = eval_stream_fn()\n",
        "print(colored('eval data (en, de) tuple:', 'red'), next(eval_stream))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[31mtrain data (en, de) tuple:\u001b[0m (b'During treatment with olanzapine, adolescents gained significantly more weight compared with adults.\\n', b'W\\xc3\\xa4hrend der Behandlung mit Olanzapin nahmen die Jugendlichen im Vergleich zu Erwachsenen signifikant mehr Gewicht zu.\\n')\n",
            "\u001b[31meval data (en, de) tuple:\u001b[0m (b'Lutropin alfa Subcutaneous use.\\n', b'Pulver zur Injektion Lutropin alfa Subkutane Anwendung\\n')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VE2t-YOTdQoV",
        "outputId": "c3f9125d-c777-415d-feba-d7be547c29fc",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-d5c48914-aac8-45e8-8380-268af8a57583\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-d5c48914-aac8-45e8-8380-268af8a57583\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving ende_32k.subword to ende_32k.subword\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0uJR0fWWECu"
      },
      "source": [
        "vocab_file = 'ende_32k.subword'\n",
        "vocab_dir = 'dir/'\n",
        "\n",
        "#tokenize the dataset\n",
        "tokenized_train_stream = trax.data.Tokenize(vocab_file = vocab_file, vocab_dir = vocab_dir)(train_stream)\n",
        "tokenized_eval_stream = trax.data.Tokenize(vocab_file = vocab_file, vocab_dir = vocab_dir)(eval_stream)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQEfvvWdc4_8"
      },
      "source": [
        "# **Tokenization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXW4cAl1WEFM",
        "outputId": "055e2b46-80bc-4638-d053-91cec4c89b5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        }
      },
      "source": [
        "#Append the EOS at the end of each sentence\n",
        "EOS = 1\n",
        "\n",
        "def append_eos(stream):\n",
        "  for (inputs, targets) in stream:\n",
        "    inputs_with_eos = list(inputs) + [EOS]\n",
        "    targets_with_eos = list(targets) + [EOS]\n",
        "    yield np.array(inputs_with_eos), np.array(targets_with_eos)\n",
        "\n",
        "#append EOS to training data and validation data\n",
        "tokenized_train_stream = append_eos(tokenized_train_stream)\n",
        "tokenized_eval_stream = append_eos(tokenized_eval_stream)\n",
        "\n",
        "train_inputs, train_targets = next(tokenized_train_stream)\n",
        "\n",
        "#print the tokenized sentences from training\n",
        "print('Input sentence from training:', train_inputs)\n",
        "print('Target sentence from training:', train_targets, '\\n')\n",
        "\n",
        "eval_inputs, eval_targets = next(tokenized_eval_stream)\n",
        "\n",
        "#print the tokenized sentences from validation\n",
        "print('Input sentence from validation:', eval_inputs)\n",
        "print('Target sentence from validation:', eval_targets)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input sentence from training: [ 5345   568   909 30650  4048  5701  3771   115   349  9935   115  8035\n",
            "    16 10146  4644    36   909 30650  4048  5701  3771   115   135   208\n",
            "     8   909 33287   913   349  9935  3550 30650  4729   992     1     1]\n",
            "Target sentence from training: [ 4172  2020  6006   349  9935   115 18457     5    24  6438  7368    69\n",
            "  6006   135   208    12   909 33287   913   349  9935  3550 30650  4729\n",
            "   992     1     1] \n",
            "\n",
            "Input sentence from validation: [  118    16  9000    17     4  6826  7211  3853  8834 20293  1978     7\n",
            " 17067 13658    23  9708 12106   596    16   615    15 19849  3550 30650\n",
            "  4729   992     1     1]\n",
            "Target sentence from validation: [  168    78    41    44   159 23385     5 15881 23486  6469  7174    11\n",
            "  7975 13658    23    15 12718 16754   596  2850  3550 30650  4729   992\n",
            "     1     1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0pgGj2V8WEI5",
        "outputId": "ad493d73-bfab-4090-827b-fdd043399714",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        }
      },
      "source": [
        "#Filter too long sentences to not run out of memory\n",
        "#length_keys = [0,1] means filter both english and german sentences, 256 tokens for training and 512 tokens for eval\n",
        "filtered_train_stream = trax.data.FilterByLength(max_length=256, length_keys=[0,1])(tokenized_train_stream)\n",
        "filtered_eval_stream = trax.data.FilterByLength(max_length=512, length_keys=[0,1])(tokenized_eval_stream)\n",
        "\n",
        "train_inputs1, train_targets1 = next(filtered_train_stream)\n",
        "\n",
        "#print the tokenized sentences from training\n",
        "print('Filtered Input sentence from training:', train_inputs)\n",
        "print('Filtered Target sentence from training:', train_targets, '\\n')\n",
        "\n",
        "eval_inputs1, eval_targets1 = next(filtered_eval_stream)\n",
        "\n",
        "#print the tokenized sentences from validation\n",
        "print('Filtered Input sentence from validation:', eval_inputs)\n",
        "print('Filtered Target sentence from validation:', eval_targets)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Filtered Input sentence from training: [ 5345   568   909 30650  4048  5701  3771   115   349  9935   115  8035\n",
            "    16 10146  4644    36   909 30650  4048  5701  3771   115   135   208\n",
            "     8   909 33287   913   349  9935  3550 30650  4729   992     1     1]\n",
            "Filtered Target sentence from training: [ 4172  2020  6006   349  9935   115 18457     5    24  6438  7368    69\n",
            "  6006   135   208    12   909 33287   913   349  9935  3550 30650  4729\n",
            "   992     1     1] \n",
            "\n",
            "Filtered Input sentence from validation: [  118    16  9000    17     4  6826  7211  3853  8834 20293  1978     7\n",
            " 17067 13658    23  9708 12106   596    16   615    15 19849  3550 30650\n",
            "  4729   992     1     1]\n",
            "Filtered Target sentence from validation: [  168    78    41    44   159 23385     5 15881 23486  6469  7174    11\n",
            "  7975 13658    23    15 12718 16754   596  2850  3550 30650  4729   992\n",
            "     1     1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_zvh-ICdPNw"
      },
      "source": [
        "# **Tokenize and detokenize helper function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxUwR8uQdQqs"
      },
      "source": [
        "#encodes a string to array of numbers\n",
        "def tokenize(input_str, vocab_file = None, vocab_dir=None):\n",
        "\n",
        "  EOS = 1\n",
        "\n",
        "  inputs = next(trax.data.tokenize(iter([input_str]), vocab_file=vocab_file, vocab_dir=vocab_dir))\n",
        "  inputs = list(inputs) + [EOS]\n",
        "\n",
        "  #adding the batch dimension to the front of the shape\n",
        "  batch_inputs = np.reshape(np.array(inputs), [1, -1])\n",
        "\n",
        "  return batch_inputs\n",
        "\n",
        "def detokenize(integers, vocab_file = None, vocab_dir=None):\n",
        "\n",
        "  #remove the dimension of size 1\n",
        "  integers = list(np.squeeze(integers))\n",
        "\n",
        "  EOS = 1\n",
        "\n",
        "  #remove the EOS to decode only the original tokens\n",
        "  if EOS in integers:\n",
        "    integers = integers[:integers.index(EOS)]\n",
        "  \n",
        "  return trax.data.detokenize(integers, vocab_file = vocab_file, vocab_dir=vocab_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_kVedgqEcvzt",
        "outputId": "870d280a-64b4-4661-b21f-a07df8efb516",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "print('SIngle detokenized example input:', detokenize(train_inputs1, vocab_file=vocab_file, vocab_dir=vocab_dir))\n",
        "print('SIngle detokenized example target:', detokenize(train_targets1, vocab_file=vocab_file, vocab_dir=vocab_dir))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SIngle detokenized example input: Driving and using machines\n",
            "\n",
            "SIngle detokenized example target: Verkehrstüchtigkeit und das Bedienen von Maschinen\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cZN2mmViUGh"
      },
      "source": [
        "# **Bucketing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BP1uZjOdcv14"
      },
      "source": [
        "# Buckets are defined in terms of boundaries and batch sizes.\n",
        "# Batch_sizes[i] determines the batch size for items with length < boundaries[i]\n",
        "# So below, we'll take a batch of 256 sentences of length < 8, 128 if length is\n",
        "# between 8 and 16, and so on and only 2 if length is over 512.\n",
        "boundaries = [8, 16, 24, 32, 64, 128, 256, 512]\n",
        "batch_sizes = [256, 128, 64, 32, 16, 8, 4, 2]\n",
        "\n",
        "#create the generators\n",
        "train_batch_stream = trax.data.BucketByLength(boundaries, batch_sizes, length_keys= [0,1])(filtered_train_stream)\n",
        "eval_batch_stream = trax.data.BucketByLength(boundaries, batch_sizes, length_keys=[0,1])(filtered_eval_stream)\n",
        "\n",
        "#add masking for the padding 0's\n",
        "train_batch_stream = trax.data.AddLossWeights(id_to_mask=0)(train_batch_stream)\n",
        "eval_batch_stream = trax.data.AddLossWeights(id_to_mask=0)(eval_batch_stream)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNUfA2ducv5Q",
        "outputId": "213d4d34-3423-4a38-abe1-9973a58a88fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "input_batch, target_batch, mask_batch = next(train_batch_stream)\n",
        "\n",
        "print('input_batch type:', type(input_batch))\n",
        "print('target_batch type:', type(target_batch))\n",
        "\n",
        "print('input_batch shape:', input_batch.shape)\n",
        "print('target_batch shape:', target_batch.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input_batch type: <class 'numpy.ndarray'>\n",
            "target_batch type: <class 'numpy.ndarray'>\n",
            "input_batch shape: (16, 64)\n",
            "target_batch shape: (16, 64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAtSU6wpdQuB",
        "outputId": "ba2a9ebf-5b85-4ca0-c3a7-a529d11e707f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        }
      },
      "source": [
        "#pick a random index less than length of input_batch\n",
        "index = random.randrange(len(input_batch))\n",
        "\n",
        "print('This is the english sentence:', detokenize(input_batch[index], vocab_file=vocab_file, vocab_dir=vocab_dir))\n",
        "print('This is the tokenized version of the english sentence:', input_batch[index], '\\n')\n",
        "\n",
        "print('This is the german sentence:', detokenize(target_batch[index], vocab_file=vocab_file, vocab_dir=vocab_dir))\n",
        "print('This is the tokenized version of the german sentence:', target_batch[index], '\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This is the english sentence: The carcinogenic potential of methoxy polyethylene glycol-epoetin beta has not been evaluated in long-term animal studies.\n",
            "\n",
            "This is the tokenized version of the english sentence: [   29  4492 12488 16956   841  1424     7 17067  5544 14533     5 16876\n",
            " 11220 22175   510 10255 11828  6078    15  4472  6434  5193     5 11342\n",
            "    13    63    48   110 13693   103     6   326    15   601  4663  4398\n",
            "  3550 30650  4729   992     1     1     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0] \n",
            "\n",
            "This is the german sentence: Das kanzerogene Potenzial von Methoxy-Polyethylenglycol-Epoetin beta wurde nicht in Langzeitstudien an Tieren untersucht.\n",
            "\n",
            "This is the tokenized version of the german sentence: [  111  4120 26643 20171  7956    21 15946 19646   105    15 21235 11220\n",
            " 22175 28200 11828  6078    15 18585  6434  5193     5 11342    13   169\n",
            "    44     6 31731 27283    23    27  5623    28  9981  3550 30650  4729\n",
            "   992     1     1     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0] \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMSF1f1gpUAE"
      },
      "source": [
        "# **2. Scaled Dot-Product Attention**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkYkTKJjpdxB"
      },
      "source": [
        "## **2.1 Input encoder function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlBjWMXCml_L"
      },
      "source": [
        "def input_encoder_fn(input_vocab_size, d_model, n_encoder_layers):\n",
        "\n",
        "  input_encoder = tl.Serial(\n",
        "      tl.Embedding(input_vocab_size, d_model),\n",
        "      [tl.LSTM(d_model) for _ in range(n_encoder_layers)]\n",
        "\n",
        "  )\n",
        "  return input_encoder\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvkTxQ4-90xl",
        "outputId": "44759c62-cc8c-467d-8443-040c6ac93e62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import w1_unittest\n",
        "\n",
        "w1_unittest.test_input_encoder_fn(input_encoder_fn)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[92m All tests passed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CGj6-vDqn7t"
      },
      "source": [
        "## **2.2 Pre-attention decoder function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0v2MeWU8mmCY"
      },
      "source": [
        "#Pre-attention decoder runs on the targets and creates activations that are used as queries in attention.\n",
        "def pre_attention_decoder_fn(mode, target_vocab_size, d_model):\n",
        "\n",
        "  pre_attention_decoder = tl.Serial(\n",
        "      tl.ShiftRight(mode = mode),\n",
        "      tl.Embedding(target_vocab_size, d_model),\n",
        "      tl.LSTM(d_model)\n",
        "  )\n",
        "\n",
        "  return pre_attention_decoder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDWRGlif99qX",
        "outputId": "359d8a17-c2c7-4086-a477-8b4b3339a1c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "w1_unittest.test_pre_attention_decoder_fn(pre_attention_decoder_fn)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[92m All tests passed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VZc_0t2r58z"
      },
      "source": [
        "## **2.3 Prepare attention input** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSS2Rg3OmmFg"
      },
      "source": [
        "def prepare_attention_input(encoder_activations, decoder_activations, inputs):\n",
        "\n",
        "  keys = encoder_activations\n",
        "  values = encoder_activations\n",
        "\n",
        "  queries = decoder_activations\n",
        "\n",
        "  # generate the mask to distinguish real tokens from padding\n",
        "  mask = (inputs != 0)\n",
        "\n",
        "  # add axes to the mask for attention heads (attention head is 1) and decoder length\n",
        "  mask = fastnp.reshape(mask, (mask.shape[0], 1, 1, mask.shape[1]))\n",
        "\n",
        "  # broadcast so mask shape is [batch size, attention heads, decoder-len, encoder-len]\n",
        "  mask = mask + fastnp.zeros((1, 1, decoder_activations.shape[1], 1))\n",
        "\n",
        "  return queries, keys, values, mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfWqkyrX-FHW",
        "outputId": "07da0bb2-3bd6-4e8c-d46a-bc2d1acf44e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "w1_unittest.test_prepare_attention_input(prepare_attention_input)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[92m All tests passed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQHYdZt2tzR5"
      },
      "source": [
        "# **3. LSTM seq2seq model with Attention**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tagGMbrbsAgq"
      },
      "source": [
        "#Returns an LSTM sequence-to-sequence model with attention\n",
        "def NMTAttn(input_vocab_size = 33300, \n",
        "                 target_vocab_size = 33300, \n",
        "                 d_model = 1024, \n",
        "                 n_encoder_layers = 2,\n",
        "                 n_decoder_layers = 2,\n",
        "                 n_attention_heads = 4, \n",
        "                 attention_dropuout = 0.0, \n",
        "                 mode = 'train'):\n",
        "\n",
        "  # Step 0: call the helper function to create layers for the input encoder\n",
        "  input_encoder = input_encoder_fn(input_vocab_size, d_model, n_encoder_layers)\n",
        "\n",
        "  # Step 0: call the helper function to create layers for the pre-attention decoder\n",
        "  pre_attention_decoder = pre_attention_decoder_fn(mode, target_vocab_size, d_model)\n",
        "\n",
        "  # Step 1: create a serial network\n",
        "  model = tl.Serial(\n",
        "      \n",
        "      # Step 2: copy input tokens and target tokens as they will be needed later.\n",
        "      tl.Select([0, 1, 0, 1]),\n",
        "\n",
        "      # Step 3: run input encoder on the input and pre-attention decoder the target.\n",
        "      tl.Parallel(input_encoder, pre_attention_decoder),\n",
        "\n",
        "      # Step 4: prepare queries, keys, values and mask for attention.\n",
        "      tl.Fn('PrepareAttentionInput', prepare_attention_input, n_out = 4),\n",
        "\n",
        "      # Step 5: run the AttentionQKV layer and nest it inside a Residual layer to add to the pre-attention decoder activations(i.e. queries)\n",
        "      tl.Residual(tl.AttentionQKV(d_model, n_heads = n_attention_heads, dropout = attention_dropuout, mode = mode)),\n",
        "\n",
        "      # Step 6: drop attention mask\n",
        "      tl.Select([0,2]),\n",
        "\n",
        "      # Step 7: run the rest of the RNN decoder\n",
        "      [tl.LSTM(d_model) for _ in range(n_decoder_layers)],\n",
        "\n",
        "      # Step 8: prepare output by making it the right size\n",
        "      tl.Dense(target_vocab_size),\n",
        "\n",
        "      # Step 9: Log-softmax for output\n",
        "      tl.LogSoftmax()\n",
        "  )\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jiqCcB4R-JmD",
        "outputId": "44138591-5091-42c4-ee57-14d9ecc696d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "w1_unittest.test_NMTAttn(NMTAttn)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[92m All tests passed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmM2pbAOt7SS",
        "outputId": "68782581-d5ce-43c2-b644-4d244b361ca0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 657
        }
      },
      "source": [
        "model = NMTAttn()\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Serial_in2_out2[\n",
            "  Select[0,1,0,1]_in2_out4\n",
            "  Parallel_in2_out2[\n",
            "    Serial[\n",
            "      Embedding_33300_1024\n",
            "      LSTM_1024\n",
            "      LSTM_1024\n",
            "    ]\n",
            "    Serial[\n",
            "      ShiftRight(1)\n",
            "      Embedding_33300_1024\n",
            "      LSTM_1024\n",
            "    ]\n",
            "  ]\n",
            "  PrepareAttentionInput_in3_out4\n",
            "  Serial_in4_out2[\n",
            "    Branch_in4_out3[\n",
            "      None\n",
            "      Serial_in4_out2[\n",
            "        Parallel_in3_out3[\n",
            "          Dense_1024\n",
            "          Dense_1024\n",
            "          Dense_1024\n",
            "        ]\n",
            "        PureAttention_in4_out2\n",
            "        Dense_1024\n",
            "      ]\n",
            "    ]\n",
            "    Add_in2\n",
            "  ]\n",
            "  Select[0,2]_in3_out2\n",
            "  LSTM_1024\n",
            "  LSTM_1024\n",
            "  Dense_33300\n",
            "  LogSoftmax\n",
            "]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enq6NbCizX7X"
      },
      "source": [
        "# **3. Training**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECV0FUBkzgV-"
      },
      "source": [
        "### **3.1 Train Task**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J422ohVIt7Vp"
      },
      "source": [
        "train_task = training.TrainTask(\n",
        "    labeled_data = train_batch_stream,\n",
        "    loss_layer = tl.CrossEntropyLoss(),\n",
        "    optimizer = trax.optimizers.Adam(0.01),\n",
        "    # use the `trax.lr.warmup_and_rsqrt_decay` as the learning rate schedule have 1000 warmup steps with a max value of 0.01\n",
        "    lr_schedule= trax.lr.warmup_and_rsqrt_decay(1000, .01),\n",
        "    n_steps_per_checkpoint = 10,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WIIwWcDK-Y3D",
        "outputId": "e057e814-8648-47a0-f31c-fd0282054f3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "w1_unittest.test_train_task(train_task)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[92m All tests passed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-lhljAd0LTb"
      },
      "source": [
        "### **3.2 Eval Task**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PsvkBbaat7fl"
      },
      "source": [
        "eval_task = training.EvalTask(\n",
        "    labeled_data = eval_batch_stream,\n",
        "    metrics = [tl.CrossEntropyLoss(), tl.Accuracy()],\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0v6gMMNU0r4n"
      },
      "source": [
        "### **3.3 Loop**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vj9U4ydd0hFt",
        "outputId": "2d330e79-cf77-4d9e-c264-7ef465bf993e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "output_dir = '/content/model1'\n",
        " \n",
        "!rm -f ~/content/model1/model.pkl.gz  \n",
        "\n",
        "training_loop = training.Loop(\n",
        "    NMTAttn(mode = 'train'),\n",
        "    train_task,\n",
        "    eval_tasks = [eval_task],\n",
        "    output_dir = output_dir)\n",
        "\n",
        "training_loop.run(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Step      1: Ran 1 train steps in 69.40 secs\n",
            "Step      1: train CrossEntropyLoss |  10.44394588\n",
            "Step      1: eval  CrossEntropyLoss |  10.44402790\n",
            "Step      1: eval          Accuracy |  0.00000000\n",
            "\n",
            "Step     10: Ran 9 train steps in 82.39 secs\n",
            "Step     10: train CrossEntropyLoss |  10.30433750\n",
            "Step     10: eval  CrossEntropyLoss |  10.05722523\n",
            "Step     10: eval          Accuracy |  0.02896341\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OksSRFra3VXQ"
      },
      "source": [
        "#. **4. Testing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkzb3-920hIF"
      },
      "source": [
        "model = NMTAttn(mode = 'eval')\n",
        "\n",
        "model.init_from_file('/content/model1/model.pkl.gz', weights_only = True)\n",
        "model = tl.Accelerate(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvRyO64TXzpr"
      },
      "source": [
        "##**4.1 Decoding**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JD1YvSCh0hLc"
      },
      "source": [
        "#This function returns the index of the next token in the translated sentence and log probability of next symbol\n",
        "#temperature (float): parameter for sampling ranging from 0.0 to 1.0. 0.0: same as argmax, always pick the most probable token and\n",
        "#1.0: sampling from the distribution (can sometimes say random things)\n",
        "def next_symbol(model, input_tokens, cur_output_tokens, temperature):\n",
        "\n",
        "    # set the length of the current output tokens\n",
        "    token_length = len(cur_output_tokens)\n",
        "\n",
        "    # calculate next power of 2 for padding length \n",
        "    padded_length = 2**int(np.ceil(np.log2(token_length + 1)))\n",
        "\n",
        "    # pad cur_output_tokens up to the padded_length\n",
        "    padded = cur_output_tokens + [0] * (padded_length - token_length)\n",
        "  \n",
        "    # model expects the output to have an axis for the batch size in front so\n",
        "    # convert `padded` list to a numpy array with shape (x, <padded_length>) where the\n",
        "    # x position is the batch axis. (hint: you can use np.expand_dims() with axis=0 to insert a new axis)\n",
        "    padded_with_batch = np.expand_dims(padded, axis=0)\n",
        "\n",
        "    # get the model prediction. remember to use the `NMAttn` argument defined above.\n",
        "    # hint: the model accepts a tuple as input (e.g. `my_model((input1, input2))`)\n",
        "    output, _ = model((input_tokens, padded_with_batch))\n",
        "    \n",
        "    # get log probabilities from the last token output\n",
        "    log_probs = output[0, token_length, :]\n",
        "\n",
        "    # get the next symbol by getting a logsoftmax sample (*hint: cast to an int)\n",
        "    symbol = int(tl.logsoftmax_sample(log_probs, temperature))\n",
        "\n",
        "    return symbol, float(log_probs[symbol])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6y-CtrSh-ije",
        "outputId": "39502a92-33d5-4ea6-8561-0e94217b854c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "w1_unittest.test_next_symbol(next_symbol, model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Expected output:  [140, -0.000217437744]\n",
            "\u001b[92m 1  Tests passed\n",
            "\u001b[91m 1  Tests failed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HbJonXgd37IM"
      },
      "source": [
        "#This function will call the next_symbol() function several times until the next output is the end-of-sentence token(EOS)\n",
        "#It takes in an input sentence to translate and returns the translated version of that string.\n",
        "#Returns:tuple: (list, str, float)\n",
        "#list of int: tokenized version of the translated sentence\n",
        "#float: log probability of the translated sentence\n",
        "#str: the translated sentence\n",
        "#this function generates the translation by getting the most probable word at each step.\n",
        "def sampling_decode(input_sentence, model = None, temperature = 0.0, vocab_file = None, vocab_dir=None):\n",
        "\n",
        "  # encode the input sentence\n",
        "  input_tokens = tokenize(input_sentence, vocab_file, vocab_dir)\n",
        "\n",
        "  cur_output_tokens = []\n",
        "\n",
        "  # initialize an integer that represents the current output index\n",
        "  cur_output = 0\n",
        "\n",
        "  EOS = 1\n",
        "\n",
        "  # check that the current output is not the end of sentence token\n",
        "  while cur_output != EOS:\n",
        "\n",
        "    # update the current output token by getting the index of the next word\n",
        "    cur_output, log_prob = next_symbol(model, input_tokens, cur_output_tokens, temperature) \n",
        "\n",
        "    # append the current output token to the list of output tokens\n",
        "    cur_output_tokens.append(cur_output)\n",
        "\n",
        "  # detokenize the output tokens\n",
        "  sentence = detokenize(cur_output_tokens, vocab_file, vocab_dir)\n",
        "\n",
        "  return cur_output_tokens, log_prob, sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMyY60EDDElo",
        "outputId": "fbe988d5-9a90-4f6b-d60e-8465abc648c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "w1_unittest.test_sampling_decode(sampling_decode, model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test 1 fails\n",
            "Test 2 fails\n",
            "\u001b[92m 0  Tests passed\n",
            "\u001b[91m 2  Tests failed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEo2eh5urnGh",
        "outputId": "7c4e92ec-bd51-4d1a-c2de-1ebe4df536e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "sampling_decode(\"I love languages.\", model, temperature=0, vocab_file=vocab_file, vocab_dir = vocab_dir)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([752, 1], -9.70961856842041, 'Bei')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 141
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2_9ytk337Nl"
      },
      "source": [
        "#The following function returns the translated version in input sentence\n",
        "def greedy_decode_test(sentence, model = None, vocab_file = None, vocab_dir = None):\n",
        "  temperature = 0\n",
        "  _,_,translated_sentence = sampling_decode(sentence, model, temperature, vocab_file, vocab_dir)\n",
        "  print('English sentence:', sentence)\n",
        "  print('German sentence:', translated_sentence)\n",
        "  return translated_sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SrcWl08mNGH4",
        "outputId": "74b8f403-9c1d-4caa-9e01-faef5d3e7f50",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "your_sentence = 'I love languages.'\n",
        "greedy_decode_test(your_sentence, model, vocab_file=vocab_file, vocab_dir = vocab_dir);"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English sentence: I love languages.\n",
            "German sentence: Bei\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZQLQYXENJoG",
        "outputId": "4e11a94c-7030-4e80-bf61-b0d714206a37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "greedy_decode_test('You are almost done with the assignment!', model, vocab_file=vocab_file, vocab_dir = vocab_dir)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English sentence: You are almost done with the assignment!\n",
            "German sentence: Bei\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Bei'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 144
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q70ez8giX6KD"
      },
      "source": [
        "## **4.2 Minimum Bayes Risk Decoding**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FSGIEarXqU8"
      },
      "source": [
        "#getting the most probable token at each step may not necessarily produce the best results. Another approach is to do Minimum Bayes Risk Decoding or MBR. \n",
        "#The general steps to implement this are:\n",
        "#1.take several random samples\n",
        "#2.score each sample against all other samples\n",
        "#3.select the one with the highest score\n",
        "def generate_samples(sentence, n_samples, model = None, temperature = 0.6, vocab_file = None, vocab_dir = None):\n",
        "  samples, log_probs = [], []\n",
        "  for _ in range(n_samples):\n",
        "    sample, logp, _ = sampling_decode(sentence, model, temperature, vocab_file = vocab_file, vocab_dir = vocab_dir)\n",
        "    samples.append(sample)\n",
        "    log_probs.append(logp)\n",
        "  return samples, log_probs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R67u30iwaDUf"
      },
      "source": [
        "## **4.3 Jaccard Similarity to compare a sample against other sample**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iur-f2ZuXqZ-"
      },
      "source": [
        "#we will be calculating scores for unigram overlaps\n",
        "#candidate is the tokenized version of candidate translation\n",
        "#reference is the tokenized version of reference translation\n",
        "def jaccard_similarity(candidate, reference):\n",
        "  # convert the lists to a set to get the unique tokens\n",
        "  can_unigram_set, ref_unigram_set = set(candidate), set(reference)\n",
        "\n",
        "  # get the set of tokens common to both candidate and reference\n",
        "  joint_elems = can_unigram_set.intersection(ref_unigram_set)\n",
        "\n",
        "  # get the set of all tokens found in either candidate or reference\n",
        "  all_elems = can_unigram_set.union(ref_unigram_set)\n",
        "\n",
        "  overlap = len(joint_elems) / len(all_elems)\n",
        "\n",
        "  return overlap"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCvikD9db1X0"
      },
      "source": [
        "## **4.4 Rouge1 Similarity for unigrams**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m97lrN84XqdZ"
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "#Returns the ROUGE-1 score between two token lists\n",
        "#system: tokenized version of the system translation\n",
        "#reference: tokenized version of the reference trans\n",
        "def rouge1_similarity(system, reference):\n",
        "  # make a frequency table of the system tokens\n",
        "  sys_counter = Counter(system)\n",
        "\n",
        "  # make a frequency table of the reference tokens\n",
        "  ref_counter = Counter(reference)\n",
        "\n",
        "  overlap = 0\n",
        "\n",
        "  for token in sys_counter:\n",
        "    # lookup the value of the token in the sys_counter dictionary\n",
        "    token_count_system = sys_counter.get(token, 0)\n",
        "\n",
        "    # lookup the value of the token in the ref_counter dictionary\n",
        "    token_count_ref = ref_counter.get(token, 0)\n",
        "\n",
        "    # update the overlap by getting the smaller number between the two token counts above\n",
        "    overlap += min(token_count_system, token_count_ref)\n",
        "\n",
        "  # get the precision (i.e. number of overlapping tokens / number of system tokens)\n",
        "  precision = overlap / sum(sys_counter.values())\n",
        "\n",
        "  # get the recall (i.e. number of overlapping tokens / number of reference tokens)\n",
        "  recall = overlap / sum(ref_counter.values())\n",
        "\n",
        "  if precision + recall != 0:\n",
        "    # compute the f1-score\n",
        "    rouge1_score = 2 * ((precision * recall) / (precision + recall))\n",
        "  else:\n",
        "    rouge1_score = 0\n",
        "  \n",
        "  return rouge1_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ho4YHeEJeiut"
      },
      "source": [
        "## **4.5 Overall Score**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sd7NjGv-XqpX"
      },
      "source": [
        "#We will now build a function to generate the overall score for a particular sample.\n",
        "#these will be the steps to generate the scores of a 4-sample list.\n",
        "#Get similarity score between sample 1 and sample 2\n",
        "#Get similarity score between sample 1 and sample 3\n",
        "#Get similarity score between sample 1 and sample 4\n",
        "#Get average score of the first 3 steps. This will be the overall score of sample 1.\n",
        "#Iterate and repeat until samples 1 to 4 have overall scores\n",
        "#the following function Returns the arithmetic mean of each candidate sentence in the samples\n",
        "#samples (list of lists): tokenized version of the translated sentences\n",
        "#*ignore_params: additional parameters will be ignored\n",
        "def average_overlap(similarity_fn, samples, *ignore_params):\n",
        "\n",
        "  scores = {}\n",
        "\n",
        "  for index_candidate, candidate in enumerate(samples):\n",
        "    overlap = 0\n",
        "\n",
        "    for index_sample, sample in enumerate(samples):\n",
        "\n",
        "      if index_candidate == index_sample:\n",
        "        continue\n",
        "\n",
        "      # get the overlap between candidate and sample using the similarity function\n",
        "      sample_overlap = similarity(candidate, sample)\n",
        "\n",
        "      # add the sample overlap to the total overlap\n",
        "      overlap += sample_overlap\n",
        "\n",
        "    #get the score for the candidate by computing the average\n",
        "    score = overlap / index_sample\n",
        "\n",
        "    # save the score in the dictionary. use index as the key.\n",
        "    scores[index_candidate] = score\n",
        "\n",
        "  return scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9WMgI6RlhIcU"
      },
      "source": [
        "#Returns the weighted mean of each candidate sentence in the samples\n",
        "#samples: tokenized version of the translated sentences\n",
        "#log_probs (list of float): log probability of the translated sentences\n",
        "def weighted_avg_overlap(similarity_fn, samples,log_probs):\n",
        "  scores = {}\n",
        "\n",
        "  for index_candidate, candidate in enumerate(samples):\n",
        "    overlap, weights_sum = 0.0, 0.0\n",
        "\n",
        "    for index_sample, (sample, logp) in enumerate(zip(samples, log_probs)):\n",
        "\n",
        "      if index_candidate == index_sample:\n",
        "        continue\n",
        "\n",
        "      # convert log probability to linear scale\n",
        "      sample_p = float(np.exp(logp))\n",
        "\n",
        "      # update the weighted sum\n",
        "      weights_sum += sample_p\n",
        "\n",
        "      # get the unigram overlap between candidate and sample\n",
        "      sample_overlap = similarity_fn(candidate, sample)\n",
        "\n",
        "      # update the overlap\n",
        "      overlap += sample_p * sample_overlap\n",
        "\n",
        "    # get the score for the candidate\n",
        "    score = overlap / weights_sum\n",
        "\n",
        "    # save the score in the dictionary. use index as the key.\n",
        "    scores[index_candidate] = score\n",
        "\n",
        "  return scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9KjW-r4hIfU"
      },
      "source": [
        "#We will now put everything together and develop the mbr_decode() function.\n",
        "#You will want to generate samples, get the score for each sample, get the highest score among all samples, then detokenize this sample to get the translated sentence.\n",
        "#the following function Returns the translated sentence using Minimum Bayes Risk decoding\n",
        "#sentence (str): sentence to translate.\n",
        "#n_samples (int): number of samples to generate\n",
        "#score_fn (function): function that generates the score for each sample\n",
        "#similarity_fn (function): function used to compute the overlap between a pair of samples\n",
        "def mbr_decode(sentence, n_samples, score_fn, similarity_fn, model = None, temperature = 0.6, vocab_file = None, vocab_dir = None):\n",
        "  # generate samples\n",
        "  samples, log_probs = generate_samples(sentence, n_samples, model, temperature, vocab_file, vocab_dir)\n",
        "\n",
        "  # use the scoring function to get a dictionary of scores\n",
        "  scores = score_fn(similarity_fn, samples, log_probs )\n",
        "\n",
        "  # find the key with the highest score\n",
        "  max_index = max(scores, key=scores.get)\n",
        "\n",
        "  # detokenize the token list associated with the max_index\n",
        "  translated_sentence = detokenize(samples[max_index], vocab_file, vocab_dir)\n",
        "\n",
        "  return (translated_sentence, max_index, scores)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tux6zF-1Xqxs"
      },
      "source": [
        "temperature = 1.0\n",
        "\n",
        "your_sentence = 'She speaks English and German.'\n",
        "\n",
        "mbr_decode(your_sentence, 4, weighted_avg_overlap, jaccard_similarity, model, temperature, vocab_file=vocab_file, vocab_dir = vocab_dir)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bot7l4axj7Rg"
      },
      "source": [
        "mbr_decode('You have completed the assignment!', 4, average_overlap, rouge1_similarity, model, TEMPERATURE, vocab_file=vocab_file)[0]"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}