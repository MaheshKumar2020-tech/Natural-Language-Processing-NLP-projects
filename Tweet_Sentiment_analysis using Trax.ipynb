{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tweet Sentiment analysis1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "En1jlK89WstD",
        "outputId": "6a1794e0-c0ac-4faf-81be-5ae57a94fe6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "pip install trax==1.3.1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting trax==1.3.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fe/d8/ad90a5c79804561bbbc5fd65a4cb6b6e735370225e777cfc46980a9dc479/trax-1.3.1-py2.py3-none-any.whl (347kB)\n",
            "\r\u001b[K     |‚ñà                               | 10kB 18.6MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñâ                              | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñâ                             | 30kB 2.3MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñä                            | 40kB 2.6MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñä                           | 51kB 2.0MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                          | 61kB 2.3MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                         | 71kB 2.5MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                        | 81kB 2.7MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                       | 92kB 2.9MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                      | 102kB 2.8MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                     | 112kB 2.8MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                    | 122kB 2.8MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                   | 133kB 2.8MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                  | 143kB 2.8MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                 | 153kB 2.8MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                 | 163kB 2.8MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                | 174kB 2.8MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà               | 184kB 2.8MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà              | 194kB 2.8MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ             | 204kB 2.8MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ            | 215kB 2.8MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä           | 225kB 2.8MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä          | 235kB 2.8MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã         | 245kB 2.8MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã        | 256kB 2.8MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå       | 266kB 2.8MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå      | 276kB 2.8MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç     | 286kB 2.8MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 296kB 2.8MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 307kB 2.8MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 317kB 2.8MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 327kB 2.8MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 337kB 2.8MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 348kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from trax==1.3.1) (1.18.5)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (from trax==1.3.1) (0.17.2)\n",
            "Collecting t5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4f/77/c00ce95121f5b8363b880aec38fde71ecaf9b7eeb29ed8bd29fc5f5b8541/t5-0.6.4-py3-none-any.whl (163kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 163kB 8.5MB/s \n",
            "\u001b[?25hCollecting tensor2tensor\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d6/7c/9e87d30cefad5cbc390bb7f626efb3ded9b19416b8160f1a1278da81b218/tensor2tensor-1.15.7-py2.py3-none-any.whl (1.4MB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.5MB 12.3MB/s \n",
            "\u001b[?25hCollecting tensorflow-text\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/28/b2/2dbd90b93913afd07e6101b8b84327c401c394e60141c1e98590038060b3/tensorflow_text-2.3.0-cp36-cp36m-manylinux1_x86_64.whl (2.6MB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2.6MB 18.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.6/dist-packages (from trax==1.3.1) (0.10.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from trax==1.3.1) (1.4.1)\n",
            "Collecting funcsigs\n",
            "  Downloading https://files.pythonhosted.org/packages/69/cb/f5be453359271714c01b9bd06126eaf2e368f1fddfff30818754b5ac2328/funcsigs-1.0.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: gin-config in /usr/local/lib/python3.6/dist-packages (from trax==1.3.1) (0.3.0)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.6/dist-packages (from trax==1.3.1) (0.1.55)\n",
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.6/dist-packages (from trax==1.3.1) (2.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from trax==1.3.1) (1.15.0)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.6/dist-packages (from trax==1.3.1) (0.2.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym->trax==1.3.1) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.4.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym->trax==1.3.1) (1.3.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from t5->trax==1.3.1) (3.2.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from t5->trax==1.3.1) (0.22.2.post1)\n",
            "Collecting rouge-score\n",
            "  Downloading https://files.pythonhosted.org/packages/1f/56/a81022436c08b9405a5247b71635394d44fe7e1dbedc4b28c740e09c2840/rouge_score-0.0.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: babel in /usr/local/lib/python3.6/dist-packages (from t5->trax==1.3.1) (2.8.0)\n",
            "Collecting sacrebleu\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/c4/8e948f601a4f9609e8b2b58f31966cb13cf17b940b82aa3e767f01c42c52/sacrebleu-1.4.14-py3-none-any.whl (64kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 71kB 7.6MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.1MB 35.7MB/s \n",
            "\u001b[?25hCollecting tfds-nightly\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e9/b7/cd6b85f10881f7c0a7823f339718c56be49cf4f283cca37bc32163b73599/tfds_nightly-3.2.1.dev202010030109-py3-none-any.whl (3.5MB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3.5MB 47.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from t5->trax==1.3.1) (1.6.0+cu101)\n",
            "Collecting transformers>=2.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/19/22/aff234f4a841f8999e68a7a94bdd4b60b4cebcfeca5d67d61cd08c9179de/transformers-3.3.1-py3-none-any.whl (1.1MB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.1MB 47.4MB/s \n",
            "\u001b[?25hCollecting mesh-tensorflow[transformer]>=0.1.13\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/52/6b/d01247d9a9b49389599a28e17782ec1d3d45413cedbfe8e90aad46835329/mesh_tensorflow-0.1.16-py3-none-any.whl (305kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307kB 58.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from t5->trax==1.3.1) (1.1.2)\n",
            "Collecting tensorflow-probability==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3e/3a/c10b6c22320531c774402ac7186d1b673374e2a9d12502cbc8d811e4601c/tensorflow_probability-0.7.0-py2.py3-none-any.whl (981kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 983kB 48.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from tensor2tensor->trax==1.3.1) (2.23.0)\n",
            "Collecting tf-slim\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/97/b0f4a64df018ca018cc035d44f2ef08f91e2e8aa67271f6f19633a015ff7/tf_slim-1.1.0-py2.py3-none-any.whl (352kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 358kB 48.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from tensor2tensor->trax==1.3.1) (2.10.0)\n",
            "Requirement already satisfied: oauth2client in /usr/local/lib/python3.6/dist-packages (from tensor2tensor->trax==1.3.1) (4.1.3)\n",
            "Collecting gevent\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3f/92/b80b922f08f222faca53c8d278e2e612192bc74b0e1f0db2f80a6ee46982/gevent-20.9.0-cp36-cp36m-manylinux2010_x86_64.whl (5.3MB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5.3MB 40.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from tensor2tensor->trax==1.3.1) (4.1.2.30)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from tensor2tensor->trax==1.3.1) (4.41.1)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.6/dist-packages (from tensor2tensor->trax==1.3.1) (1.7.12)\n",
            "Collecting pypng\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/fb/f719f1ac965e2101aa6ea6f54ef8b40f8fbb033f6ad07c017663467f5147/pypng-0.0.20.tar.gz (649kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 655kB 47.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: dopamine-rl in /usr/local/lib/python3.6/dist-packages (from tensor2tensor->trax==1.3.1) (1.0.5)\n",
            "Collecting tensorflow-gan\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0c/2e/62922111d7d50e1900e3030764743ea7735540ce103b3ab30fd5cd2d8a2b/tensorflow_gan-2.0.0-py2.py3-none-any.whl (365kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 368kB 37.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow-addons in /usr/local/lib/python3.6/dist-packages (from tensor2tensor->trax==1.3.1) (0.8.3)\n",
            "Collecting gunicorn\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/ca/926f7cd3a2014b16870086b2d0fdc84a9e49473c68a8dff8b57f7c156f43/gunicorn-20.0.4-py2.py3-none-any.whl (77kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81kB 9.2MB/s \n",
            "\u001b[?25hCollecting bz2file\n",
            "  Downloading https://files.pythonhosted.org/packages/61/39/122222b5e85cd41c391b68a99ee296584b2a2d1d233e7ee32b4532384f2d/bz2file-0.98.tar.gz\n",
            "Collecting kfac\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1c/36/06fe2c757044bb51906fef231ac48cc5bf9a277fc9a8c7e1108d7e9e8cfd/kfac-0.2.3-py2.py3-none-any.whl (191kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 194kB 52.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from tensor2tensor->trax==1.3.1) (0.16.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.6/dist-packages (from tensor2tensor->trax==1.3.1) (1.1.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from tensor2tensor->trax==1.3.1) (7.0.0)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.6/dist-packages (from tensor2tensor->trax==1.3.1) (1.1.2)\n",
            "Requirement already satisfied: tensorflow<2.4,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-text->trax==1.3.1) (2.3.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->trax==1.3.1) (3.12.4)\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->trax==1.3.1) (20.2.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->trax==1.3.1) (1.12.1)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->trax==1.3.1) (0.3.2)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->trax==1.3.1) (0.24.0)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->trax==1.3.1) (2.3)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->trax==1.3.1) (1.1.0)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.6/dist-packages (from jax->trax==1.3.1) (3.3.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->t5->trax==1.3.1) (0.16.0)\n",
            "Requirement already satisfied: pytz>=2015.7 in /usr/local/lib/python3.6/dist-packages (from babel->t5->trax==1.3.1) (2018.9)\n",
            "Collecting portalocker\n",
            "  Downloading https://files.pythonhosted.org/packages/89/a6/3814b7107e0788040870e8825eebf214d72166adf656ba7d4bf14759a06a/portalocker-2.0.0-py2.py3-none-any.whl\n",
            "Collecting importlib-resources; python_version < \"3.9\"\n",
            "  Downloading https://files.pythonhosted.org/packages/ba/03/0f9595c0c2ef12590877f3c47e5f579759ce5caf817f8256d5dcbd8a1177/importlib_resources-3.0.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.6/dist-packages (from tfds-nightly->t5->trax==1.3.1) (0.1.5)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from tfds-nightly->t5->trax==1.3.1) (0.7)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 890kB 57.5MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.8.1.rc2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/83/8b9fccb9e48eeb575ee19179e2bdde0ee9a1904f97de5f02d19016b8804f/tokenizers-0.8.1rc2-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3.0MB 45.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers>=2.7.0->t5->trax==1.3.1) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers>=2.7.0->t5->trax==1.3.1) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers>=2.7.0->t5->trax==1.3.1) (20.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->t5->trax==1.3.1) (2.8.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability==0.7.0->tensor2tensor->trax==1.3.1) (4.4.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->tensor2tensor->trax==1.3.1) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->tensor2tensor->trax==1.3.1) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->tensor2tensor->trax==1.3.1) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->tensor2tensor->trax==1.3.1) (3.0.4)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client->tensor2tensor->trax==1.3.1) (0.4.8)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client->tensor2tensor->trax==1.3.1) (4.6)\n",
            "Requirement already satisfied: httplib2>=0.9.1 in /usr/local/lib/python3.6/dist-packages (from oauth2client->tensor2tensor->trax==1.3.1) (0.17.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client->tensor2tensor->trax==1.3.1) (0.2.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from gevent->tensor2tensor->trax==1.3.1) (50.3.0)\n",
            "Collecting zope.interface\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c5/89/1eb9dbb9e24f5e2c29ab1a88097b2f1333858aac3cd3cccc6c4c1c8ad867/zope.interface-5.1.2-cp36-cp36m-manylinux2010_x86_64.whl (236kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 245kB 59.7MB/s \n",
            "\u001b[?25hCollecting greenlet>=0.4.17; platform_python_implementation == \"CPython\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/d0/532e160c777b42f6f393f9de8c88abb8af6c892037c55e4d3a8a211324dd/greenlet-0.4.17-cp36-cp36m-manylinux1_x86_64.whl (44kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 51kB 5.9MB/s \n",
            "\u001b[?25hCollecting zope.event\n",
            "  Downloading https://files.pythonhosted.org/packages/9e/85/b45408c64f3b888976f1d5b37eed8d746b8d5729a66a49ec846fda27d371/zope.event-4.5.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client->tensor2tensor->trax==1.3.1) (0.0.4)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client->tensor2tensor->trax==1.3.1) (3.0.1)\n",
            "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client->tensor2tensor->trax==1.3.1) (1.17.2)\n",
            "Requirement already satisfied: tensorflow-hub>=0.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gan->tensor2tensor->trax==1.3.1) (0.9.0)\n",
            "Requirement already satisfied: typeguard in /usr/local/lib/python3.6/dist-packages (from tensorflow-addons->tensor2tensor->trax==1.3.1) (2.7.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.6/dist-packages (from sympy->tensor2tensor->trax==1.3.1) (1.1.0)\n",
            "Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.6/dist-packages (from flask->tensor2tensor->trax==1.3.1) (1.0.1)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from flask->tensor2tensor->trax==1.3.1) (7.1.2)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from flask->tensor2tensor->trax==1.3.1) (1.1.0)\n",
            "Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.6/dist-packages (from flask->tensor2tensor->trax==1.3.1) (2.11.2)\n",
            "Requirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text->trax==1.3.1) (2.3.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text->trax==1.3.1) (2.3.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text->trax==1.3.1) (0.3.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text->trax==1.3.1) (0.2.0)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text->trax==1.3.1) (1.6.3)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text->trax==1.3.1) (0.35.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text->trax==1.3.1) (1.32.0)\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text->trax==1.3.1) (1.1.2)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-metadata->tensorflow-datasets->trax==1.3.1) (1.52.0)\n",
            "Requirement already satisfied: zipp>=0.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-resources; python_version < \"3.9\"->tfds-nightly->t5->trax==1.3.1) (3.2.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers>=2.7.0->t5->trax==1.3.1) (2.4.7)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client->tensor2tensor->trax==1.3.1) (4.1.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.10.1->flask->tensor2tensor->trax==1.3.1) (1.1.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->trax==1.3.1) (0.4.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->trax==1.3.1) (3.2.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->trax==1.3.1) (1.7.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->trax==1.3.1) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->trax==1.3.1) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->trax==1.3.1) (3.1.0)\n",
            "Building wheels for collected packages: pypng, bz2file, sacremoses\n",
            "  Building wheel for pypng (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypng: filename=pypng-0.0.20-cp36-none-any.whl size=67162 sha256=bd55d8be6bb326b37c4529dfe6af8547b9ca0622b01193c9559262503f89eaba\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/6b/ef/0493b536b6d4722c2ae9486691b1d49b922b9877922beeabb3\n",
            "  Building wheel for bz2file (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bz2file: filename=bz2file-0.98-cp36-none-any.whl size=6884 sha256=2ac09c99e1858b7bce4a42ee4d559815a7879b5ff4ce95886f886b7ad6218a44\n",
            "  Stored in directory: /root/.cache/pip/wheels/81/75/d6/e1317bf09bf1af5a30befc2a007869fa6e1f516b8f7c591cb9\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=89d69acb43aec2678faa68e6b03ac9e5cce0522b52f7bd66635371fa6a4abecd\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built pypng bz2file sacremoses\n",
            "\u001b[31mERROR: kfac 0.2.3 has requirement tensorflow-probability==0.8, but you'll have tensorflow-probability 0.7.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: rouge-score, portalocker, sacrebleu, sentencepiece, tensorflow-text, importlib-resources, tfds-nightly, sacremoses, tokenizers, transformers, mesh-tensorflow, t5, tensorflow-probability, tf-slim, zope.interface, greenlet, zope.event, gevent, pypng, tensorflow-gan, gunicorn, bz2file, kfac, tensor2tensor, funcsigs, trax\n",
            "  Found existing installation: tensorflow-probability 0.11.0\n",
            "    Uninstalling tensorflow-probability-0.11.0:\n",
            "      Successfully uninstalled tensorflow-probability-0.11.0\n",
            "Successfully installed bz2file-0.98 funcsigs-1.0.2 gevent-20.9.0 greenlet-0.4.17 gunicorn-20.0.4 importlib-resources-3.0.0 kfac-0.2.3 mesh-tensorflow-0.1.16 portalocker-2.0.0 pypng-0.0.20 rouge-score-0.0.4 sacrebleu-1.4.14 sacremoses-0.0.43 sentencepiece-0.1.91 t5-0.6.4 tensor2tensor-1.15.7 tensorflow-gan-2.0.0 tensorflow-probability-0.7.0 tensorflow-text-2.3.0 tf-slim-1.1.0 tfds-nightly-3.2.1.dev202010030109 tokenizers-0.8.1rc2 transformers-3.3.1 trax-1.3.1 zope.event-4.5.0 zope.interface-5.1.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bzan1aCQW5H-",
        "outputId": "55e5b747-25e3-447d-b2cd-c2a27170c165",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "import os\n",
        "import random as rnd\n",
        "import trax\n",
        "from trax import layers as tl\n",
        "import io\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "trax.supervised.trainer_lib.init_random_number_generators(31)\n",
        "import trax.fastmath.numpy as np\n",
        "from trax import layers as tl\n",
        "from google.colab import files\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import string\n",
        "import nltk\n",
        "import random as rnd\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import TweetTokenizer \n",
        "nltk.download('stopwords')\n",
        "nltk.download('twitter_samples')\n",
        "from nltk.corpus import twitter_samples \n",
        "import re\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.tokenize import word_tokenize,RegexpTokenizer\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Package twitter_samples is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NO5IhdXWrxkO"
      },
      "source": [
        "# **1. Loading the tweet data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6dvZ2MoW5Nf",
        "outputId": "433a35bd-e2e1-40d0-fde6-992e74ea3e24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
        "all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
        "print('There are {} positive tweets'.format(len(all_positive_tweets)))\n",
        "print('There are {} negative tweets'.format(len(all_negative_tweets)))\n",
        "\n",
        "#split the positive tweets into train and validation set\n",
        "val_pos = all_positive_tweets[4000:]\n",
        "train_pos = all_positive_tweets[:4000]\n",
        "\n",
        "#split the negative tweets into train and validation set\n",
        "val_neg = all_negative_tweets[4000:]\n",
        "train_neg = all_negative_tweets[:4000]\n",
        "\n",
        "#combine train_pos and train_neg\n",
        "train_x = train_pos + train_neg\n",
        "\n",
        "#combine test_pos and test_neg\n",
        "val_x = val_pos + val_neg\n",
        "\n",
        "#set the labels for the training set (1 for positive and 0 for negative)\n",
        "train_y = np.append(np.ones(len(train_pos)), np.zeros(len(train_neg)))\n",
        "\n",
        "#set the labels for the test set (1 for positive and 0 for negative)\n",
        "val_y = np.append(np.ones(len(val_pos)), np.zeros(len(val_neg)))\n",
        "\n",
        "print('Length of training set:', len(train_x))\n",
        "print('Length of validation set:', len(val_x))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 5000 positive tweets\n",
            "There are 5000 negative tweets\n",
            "Length of training set: 8000\n",
            "Length of validation set: 2000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5yhaqNBr3UX"
      },
      "source": [
        "# **2. Function to remove unwanted characters and returns tokenized list of words**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-EHwRdXW5QL"
      },
      "source": [
        "def process_tweet(tweet):\n",
        "    stemmer = PorterStemmer()\n",
        "    stopwords_english = stopwords.words('english')\n",
        "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
        "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
        "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
        "    tweet = re.sub(r'#', '', tweet)\n",
        "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,reduce_len=True)\n",
        "    tweet_tokens = tokenizer.tokenize(tweet)\n",
        "\n",
        "    tweets_clean = []\n",
        "    for word in tweet_tokens:\n",
        "        if (word not in stopwords_english and  \n",
        "                word not in string.punctuation): \n",
        "            stem_word = stemmer.stem(word)  # stemming word\n",
        "            tweets_clean.append(stem_word)\n",
        "\n",
        "    return tweets_clean\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SY9ZDf7W5S5",
        "outputId": "6af0745e-13d5-4a52-9720-06b68a9f72af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "print('Original positive tweet in training set at position 0 is:', train_pos[0], '\\n')\n",
        "print('Preprocessed positive tweet:', process_tweet(train_pos[0]), '\\n')\n",
        "\n",
        "print('Original negative tweet in training set at position 0 is:', train_neg[0], '\\n')\n",
        "print('Preprocessed negative tweet:', process_tweet(train_neg[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original positive tweet in training set at position 0 is: #FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :) \n",
            "\n",
            "Preprocessed positive tweet: ['followfriday', 'top', 'engag', 'member', 'commun', 'week', ':)'] \n",
            "\n",
            "Original negative tweet in training set at position 0 is: hopeless for tmr :( \n",
            "\n",
            "Prprocessed negative tweet: ['hopeless', 'tmr', ':(']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K18gbvhZrmh5"
      },
      "source": [
        "# **3. Building the vocabulary**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wc4HV-w4W5Va",
        "outputId": "b58e9ef8-7a9a-4ea4-bb88-03bf07502a99",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "vocab = {'__PAD__': 0, '__</e>__': 1, '__UNK__': 2}\n",
        "#build vocabulary using training data\n",
        "for tweet in train_x:\n",
        "  processed_tweet = process_tweet(tweet)\n",
        "  for word in processed_tweet:\n",
        "    if word not in vocab:\n",
        "      vocab[word] = len(vocab)\n",
        "\n",
        "print('There are {} words in vocabulary'.format(len(vocab)))\n",
        "display(vocab)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 9092 words in vocabulary\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "{'__PAD__': 0,\n",
              " '__</e>__': 1,\n",
              " '__UNK__': 2,\n",
              " 'followfriday': 3,\n",
              " 'top': 4,\n",
              " 'engag': 5,\n",
              " 'member': 6,\n",
              " 'commun': 7,\n",
              " 'week': 8,\n",
              " ':)': 9,\n",
              " 'hey': 10,\n",
              " 'jame': 11,\n",
              " 'odd': 12,\n",
              " ':/': 13,\n",
              " 'pleas': 14,\n",
              " 'call': 15,\n",
              " 'contact': 16,\n",
              " 'centr': 17,\n",
              " '02392441234': 18,\n",
              " 'abl': 19,\n",
              " 'assist': 20,\n",
              " 'mani': 21,\n",
              " 'thank': 22,\n",
              " 'listen': 23,\n",
              " 'last': 24,\n",
              " 'night': 25,\n",
              " 'bleed': 26,\n",
              " 'amaz': 27,\n",
              " 'track': 28,\n",
              " 'scotland': 29,\n",
              " 'congrat': 30,\n",
              " 'yeaaah': 31,\n",
              " 'yipppi': 32,\n",
              " 'accnt': 33,\n",
              " 'verifi': 34,\n",
              " 'rqst': 35,\n",
              " 'succeed': 36,\n",
              " 'got': 37,\n",
              " 'blue': 38,\n",
              " 'tick': 39,\n",
              " 'mark': 40,\n",
              " 'fb': 41,\n",
              " 'profil': 42,\n",
              " '15': 43,\n",
              " 'day': 44,\n",
              " 'one': 45,\n",
              " 'irresist': 46,\n",
              " 'flipkartfashionfriday': 47,\n",
              " 'like': 48,\n",
              " 'keep': 49,\n",
              " 'love': 50,\n",
              " 'custom': 51,\n",
              " 'wait': 52,\n",
              " 'long': 53,\n",
              " 'hope': 54,\n",
              " 'enjoy': 55,\n",
              " 'happi': 56,\n",
              " 'friday': 57,\n",
              " 'lwwf': 58,\n",
              " 'second': 59,\n",
              " 'thought': 60,\n",
              " '‚Äô': 61,\n",
              " 'enough': 62,\n",
              " 'time': 63,\n",
              " 'dd': 64,\n",
              " 'new': 65,\n",
              " 'short': 66,\n",
              " 'enter': 67,\n",
              " 'system': 68,\n",
              " 'sheep': 69,\n",
              " 'must': 70,\n",
              " 'buy': 71,\n",
              " 'jgh': 72,\n",
              " 'go': 73,\n",
              " 'bayan': 74,\n",
              " ':D': 75,\n",
              " 'bye': 76,\n",
              " 'act': 77,\n",
              " 'mischiev': 78,\n",
              " 'etl': 79,\n",
              " 'layer': 80,\n",
              " 'in-hous': 81,\n",
              " 'wareh': 82,\n",
              " 'app': 83,\n",
              " 'katamari': 84,\n",
              " 'well': 85,\n",
              " '‚Ä¶': 86,\n",
              " 'name': 87,\n",
              " 'impli': 88,\n",
              " ':p': 89,\n",
              " 'influenc': 90,\n",
              " 'big': 91,\n",
              " '...': 92,\n",
              " 'juici': 93,\n",
              " 'selfi': 94,\n",
              " 'follow': 95,\n",
              " 'perfect': 96,\n",
              " 'alreadi': 97,\n",
              " 'know': 98,\n",
              " \"what'\": 99,\n",
              " 'great': 100,\n",
              " 'opportun': 101,\n",
              " 'junior': 102,\n",
              " 'triathlet': 103,\n",
              " 'age': 104,\n",
              " '12': 105,\n",
              " '13': 106,\n",
              " 'gatorad': 107,\n",
              " 'seri': 108,\n",
              " 'get': 109,\n",
              " 'entri': 110,\n",
              " 'lay': 111,\n",
              " 'greet': 112,\n",
              " 'card': 113,\n",
              " 'rang': 114,\n",
              " 'print': 115,\n",
              " 'today': 116,\n",
              " 'job': 117,\n",
              " ':-)': 118,\n",
              " \"friend'\": 119,\n",
              " 'lunch': 120,\n",
              " 'yummm': 121,\n",
              " 'nostalgia': 122,\n",
              " 'tb': 123,\n",
              " 'ku': 124,\n",
              " 'id': 125,\n",
              " 'conflict': 126,\n",
              " 'help': 127,\n",
              " \"here'\": 128,\n",
              " 'screenshot': 129,\n",
              " 'work': 130,\n",
              " 'hi': 131,\n",
              " 'liv': 132,\n",
              " 'hello': 133,\n",
              " 'need': 134,\n",
              " 'someth': 135,\n",
              " 'u': 136,\n",
              " 'fm': 137,\n",
              " 'twitter': 138,\n",
              " '‚Äî': 139,\n",
              " 'sure': 140,\n",
              " 'thing': 141,\n",
              " 'dm': 142,\n",
              " 'x': 143,\n",
              " \"i'v\": 144,\n",
              " 'heard': 145,\n",
              " 'four': 146,\n",
              " 'season': 147,\n",
              " 'pretti': 148,\n",
              " 'dope': 149,\n",
              " 'penthous': 150,\n",
              " 'obv': 151,\n",
              " 'gobigorgohom': 152,\n",
              " 'fun': 153,\n",
              " \"y'all\": 154,\n",
              " 'yeah': 155,\n",
              " 'suppos': 156,\n",
              " 'lol': 157,\n",
              " 'chat': 158,\n",
              " 'bit': 159,\n",
              " 'youth': 160,\n",
              " 'üíÖ': 161,\n",
              " 'üèΩ': 162,\n",
              " 'üíã': 163,\n",
              " 'seen': 164,\n",
              " 'year': 165,\n",
              " 'rest': 166,\n",
              " 'goe': 167,\n",
              " 'quickli': 168,\n",
              " 'bed': 169,\n",
              " 'music': 170,\n",
              " 'fix': 171,\n",
              " 'dream': 172,\n",
              " 'spiritu': 173,\n",
              " 'ritual': 174,\n",
              " 'festiv': 175,\n",
              " 'n√©pal': 176,\n",
              " 'begin': 177,\n",
              " 'line-up': 178,\n",
              " 'left': 179,\n",
              " 'see': 180,\n",
              " 'sarah': 181,\n",
              " 'send': 182,\n",
              " 'us': 183,\n",
              " 'email': 184,\n",
              " 'bitsy@bitdefender.com': 185,\n",
              " \"we'll\": 186,\n",
              " 'asap': 187,\n",
              " 'kik': 188,\n",
              " 'hatessuc': 189,\n",
              " '32429': 190,\n",
              " 'kikm': 191,\n",
              " 'lgbt': 192,\n",
              " 'tinder': 193,\n",
              " 'nsfw': 194,\n",
              " 'akua': 195,\n",
              " 'cumshot': 196,\n",
              " 'come': 197,\n",
              " 'hous': 198,\n",
              " 'nsn_supplement': 199,\n",
              " 'effect': 200,\n",
              " 'press': 201,\n",
              " 'releas': 202,\n",
              " 'distribut': 203,\n",
              " 'result': 204,\n",
              " 'link': 205,\n",
              " 'remov': 206,\n",
              " 'pressreleas': 207,\n",
              " 'newsdistribut': 208,\n",
              " 'bam': 209,\n",
              " 'bestfriend': 210,\n",
              " 'lot': 211,\n",
              " 'warsaw': 212,\n",
              " '<3': 213,\n",
              " 'x46': 214,\n",
              " 'everyon': 215,\n",
              " 'watch': 216,\n",
              " 'documentari': 217,\n",
              " 'earthl': 218,\n",
              " 'youtub': 219,\n",
              " 'support': 220,\n",
              " 'buuut': 221,\n",
              " 'oh': 222,\n",
              " 'look': 223,\n",
              " 'forward': 224,\n",
              " 'visit': 225,\n",
              " 'next': 226,\n",
              " 'letsgetmessi': 227,\n",
              " 'jo': 228,\n",
              " 'make': 229,\n",
              " 'feel': 230,\n",
              " 'better': 231,\n",
              " 'never': 232,\n",
              " 'anyon': 233,\n",
              " 'kpop': 234,\n",
              " 'flesh': 235,\n",
              " 'good': 236,\n",
              " 'girl': 237,\n",
              " 'best': 238,\n",
              " 'wish': 239,\n",
              " 'reason': 240,\n",
              " 'epic': 241,\n",
              " 'soundtrack': 242,\n",
              " 'shout': 243,\n",
              " 'ad': 244,\n",
              " 'video': 245,\n",
              " 'playlist': 246,\n",
              " 'would': 247,\n",
              " 'dear': 248,\n",
              " 'jordan': 249,\n",
              " 'okay': 250,\n",
              " 'fake': 251,\n",
              " 'gameplay': 252,\n",
              " ';)': 253,\n",
              " 'haha': 254,\n",
              " 'im': 255,\n",
              " 'kid': 256,\n",
              " 'stuff': 257,\n",
              " 'exactli': 258,\n",
              " 'product': 259,\n",
              " 'line': 260,\n",
              " 'etsi': 261,\n",
              " 'shop': 262,\n",
              " 'check': 263,\n",
              " 'vacat': 264,\n",
              " 'recharg': 265,\n",
              " 'normal': 266,\n",
              " 'charger': 267,\n",
              " 'asleep': 268,\n",
              " 'talk': 269,\n",
              " 'sooo': 270,\n",
              " 'someon': 271,\n",
              " 'text': 272,\n",
              " 'ye': 273,\n",
              " 'bet': 274,\n",
              " \"he'll\": 275,\n",
              " 'fit': 276,\n",
              " 'hear': 277,\n",
              " 'speech': 278,\n",
              " 'piti': 279,\n",
              " 'green': 280,\n",
              " 'garden': 281,\n",
              " 'midnight': 282,\n",
              " 'sun': 283,\n",
              " 'beauti': 284,\n",
              " 'canal': 285,\n",
              " 'dasvidaniya': 286,\n",
              " 'till': 287,\n",
              " 'scout': 288,\n",
              " 'sg': 289,\n",
              " 'futur': 290,\n",
              " 'wlan': 291,\n",
              " 'pro': 292,\n",
              " 'confer': 293,\n",
              " 'asia': 294,\n",
              " 'chang': 295,\n",
              " 'lollipop': 296,\n",
              " 'üç≠': 297,\n",
              " 'nez': 298,\n",
              " 'agnezmo': 299,\n",
              " 'oley': 300,\n",
              " 'mama': 301,\n",
              " 'stand': 302,\n",
              " 'stronger': 303,\n",
              " 'god': 304,\n",
              " 'misti': 305,\n",
              " 'babi': 306,\n",
              " 'cute': 307,\n",
              " 'woohoo': 308,\n",
              " \"can't\": 309,\n",
              " 'sign': 310,\n",
              " 'yet': 311,\n",
              " 'still': 312,\n",
              " 'think': 313,\n",
              " 'mka': 314,\n",
              " 'liam': 315,\n",
              " 'access': 316,\n",
              " 'welcom': 317,\n",
              " 'stat': 318,\n",
              " 'arriv': 319,\n",
              " '1': 320,\n",
              " 'unfollow': 321,\n",
              " 'via': 322,\n",
              " 'surpris': 323,\n",
              " 'figur': 324,\n",
              " 'happybirthdayemilybett': 325,\n",
              " 'sweet': 326,\n",
              " 'talent': 327,\n",
              " '2': 328,\n",
              " 'plan': 329,\n",
              " 'drain': 330,\n",
              " 'gotta': 331,\n",
              " 'timezon': 332,\n",
              " 'parent': 333,\n",
              " 'proud': 334,\n",
              " 'least': 335,\n",
              " 'mayb': 336,\n",
              " 'sometim': 337,\n",
              " 'grade': 338,\n",
              " 'al': 339,\n",
              " 'grand': 340,\n",
              " 'manila_bro': 341,\n",
              " 'chosen': 342,\n",
              " 'let': 343,\n",
              " 'around': 344,\n",
              " '..': 345,\n",
              " 'side': 346,\n",
              " 'world': 347,\n",
              " 'eh': 348,\n",
              " 'take': 349,\n",
              " 'care': 350,\n",
              " 'final': 351,\n",
              " 'fuck': 352,\n",
              " 'weekend': 353,\n",
              " 'real': 354,\n",
              " 'x45': 355,\n",
              " 'join': 356,\n",
              " 'hushedcallwithfraydo': 357,\n",
              " 'gift': 358,\n",
              " 'yeahhh': 359,\n",
              " 'hushedpinwithsammi': 360,\n",
              " 'event': 361,\n",
              " 'might': 362,\n",
              " 'luv': 363,\n",
              " 'realli': 364,\n",
              " 'appreci': 365,\n",
              " 'share': 366,\n",
              " 'wow': 367,\n",
              " 'tom': 368,\n",
              " 'gym': 369,\n",
              " 'monday': 370,\n",
              " 'invit': 371,\n",
              " 'scope': 372,\n",
              " 'friend': 373,\n",
              " 'nude': 374,\n",
              " 'sleep': 375,\n",
              " 'birthday': 376,\n",
              " 'want': 377,\n",
              " 't-shirt': 378,\n",
              " 'cool': 379,\n",
              " 'haw': 380,\n",
              " 'phela': 381,\n",
              " 'mom': 382,\n",
              " 'obvious': 383,\n",
              " 'princ': 384,\n",
              " 'charm': 385,\n",
              " 'stage': 386,\n",
              " 'luck': 387,\n",
              " 'tyler': 388,\n",
              " 'hipster': 389,\n",
              " 'glass': 390,\n",
              " 'marti': 391,\n",
              " 'glad': 392,\n",
              " 'done': 393,\n",
              " 'afternoon': 394,\n",
              " 'read': 395,\n",
              " 'kahfi': 396,\n",
              " 'finish': 397,\n",
              " 'ohmyg': 398,\n",
              " 'yaya': 399,\n",
              " 'dub': 400,\n",
              " 'stalk': 401,\n",
              " 'ig': 402,\n",
              " 'gondooo': 403,\n",
              " 'moo': 404,\n",
              " 'tologooo': 405,\n",
              " 'becom': 406,\n",
              " 'detail': 407,\n",
              " 'zzz': 408,\n",
              " 'xx': 409,\n",
              " 'physiotherapi': 410,\n",
              " 'hashtag': 411,\n",
              " 'üí™': 412,\n",
              " 'monica': 413,\n",
              " 'miss': 414,\n",
              " 'sound': 415,\n",
              " 'morn': 416,\n",
              " \"that'\": 417,\n",
              " 'x43': 418,\n",
              " 'definit': 419,\n",
              " 'tri': 420,\n",
              " 'tonight': 421,\n",
              " 'took': 422,\n",
              " 'advic': 423,\n",
              " 'treviso': 424,\n",
              " 'concert': 425,\n",
              " 'citi': 426,\n",
              " 'countri': 427,\n",
              " \"i'll\": 428,\n",
              " 'start': 429,\n",
              " 'fine': 430,\n",
              " 'gorgeou': 431,\n",
              " 'xo': 432,\n",
              " 'oven': 433,\n",
              " 'roast': 434,\n",
              " 'garlic': 435,\n",
              " 'oliv': 436,\n",
              " 'oil': 437,\n",
              " 'dri': 438,\n",
              " 'tomato': 439,\n",
              " 'basil': 440,\n",
              " 'centuri': 441,\n",
              " 'tuna': 442,\n",
              " 'right': 443,\n",
              " 'back': 444,\n",
              " 'atchya': 445,\n",
              " 'even': 446,\n",
              " 'almost': 447,\n",
              " 'chanc': 448,\n",
              " 'cheer': 449,\n",
              " 'po': 450,\n",
              " 'ice': 451,\n",
              " 'cream': 452,\n",
              " 'agre': 453,\n",
              " '100': 454,\n",
              " 'heheheh': 455,\n",
              " 'that': 456,\n",
              " 'point': 457,\n",
              " 'stay': 458,\n",
              " 'home': 459,\n",
              " 'soon': 460,\n",
              " 'promis': 461,\n",
              " 'web': 462,\n",
              " 'whatsapp': 463,\n",
              " 'volta': 464,\n",
              " 'funcionar': 465,\n",
              " 'com': 466,\n",
              " 'iphon': 467,\n",
              " 'jailbroken': 468,\n",
              " 'later': 469,\n",
              " '34': 470,\n",
              " 'min': 471,\n",
              " 'leia': 472,\n",
              " 'appear': 473,\n",
              " 'hologram': 474,\n",
              " 'r2d2': 475,\n",
              " 'w': 476,\n",
              " 'messag': 477,\n",
              " 'obi': 478,\n",
              " 'wan': 479,\n",
              " 'sit': 480,\n",
              " 'luke': 481,\n",
              " 'inter': 482,\n",
              " '3': 483,\n",
              " 'ucl': 484,\n",
              " 'arsen': 485,\n",
              " 'small': 486,\n",
              " 'team': 487,\n",
              " 'pass': 488,\n",
              " 'üöÇ': 489,\n",
              " 'dewsburi': 490,\n",
              " 'railway': 491,\n",
              " 'station': 492,\n",
              " 'dew': 493,\n",
              " 'west': 494,\n",
              " 'yorkshir': 495,\n",
              " '430': 496,\n",
              " 'smh': 497,\n",
              " '9:25': 498,\n",
              " 'live': 499,\n",
              " 'strang': 500,\n",
              " 'imagin': 501,\n",
              " 'megan': 502,\n",
              " 'masaantoday': 503,\n",
              " 'a4': 504,\n",
              " 'shweta': 505,\n",
              " 'tripathi': 506,\n",
              " '5': 507,\n",
              " '20': 508,\n",
              " 'kurta': 509,\n",
              " 'half': 510,\n",
              " 'number': 511,\n",
              " 'wsalelov': 512,\n",
              " 'ah': 513,\n",
              " 'larri': 514,\n",
              " 'anyway': 515,\n",
              " 'kinda': 516,\n",
              " 'goood': 517,\n",
              " 'life': 518,\n",
              " 'enn': 519,\n",
              " 'could': 520,\n",
              " 'warmup': 521,\n",
              " '15th': 522,\n",
              " 'bath': 523,\n",
              " 'dum': 524,\n",
              " 'andar': 525,\n",
              " 'ram': 526,\n",
              " 'sampath': 527,\n",
              " 'sona': 528,\n",
              " 'mohapatra': 529,\n",
              " 'samantha': 530,\n",
              " 'edward': 531,\n",
              " 'mein': 532,\n",
              " 'tulan': 533,\n",
              " 'razi': 534,\n",
              " 'wah': 535,\n",
              " 'josh': 536,\n",
              " 'alway': 537,\n",
              " 'smile': 538,\n",
              " 'pictur': 539,\n",
              " '16.20': 540,\n",
              " 'giveitup': 541,\n",
              " 'given': 542,\n",
              " 'ga': 543,\n",
              " 'subsidi': 544,\n",
              " 'initi': 545,\n",
              " 'propos': 546,\n",
              " 'delight': 547,\n",
              " 'yesterday': 548,\n",
              " 'x42': 549,\n",
              " 'lmaoo': 550,\n",
              " 'song': 551,\n",
              " 'ever': 552,\n",
              " 'shall': 553,\n",
              " 'littl': 554,\n",
              " 'throwback': 555,\n",
              " 'outli': 556,\n",
              " 'island': 557,\n",
              " 'cheung': 558,\n",
              " 'chau': 559,\n",
              " 'mui': 560,\n",
              " 'wo': 561,\n",
              " 'total': 562,\n",
              " 'differ': 563,\n",
              " 'kfckitchentour': 564,\n",
              " 'kitchen': 565,\n",
              " 'clean': 566,\n",
              " \"i'm\": 567,\n",
              " 'cusp': 568,\n",
              " 'test': 569,\n",
              " 'water': 570,\n",
              " 'reward': 571,\n",
              " 'arummzz': 572,\n",
              " \"let'\": 573,\n",
              " 'drive': 574,\n",
              " 'travel': 575,\n",
              " 'yogyakarta': 576,\n",
              " 'jeep': 577,\n",
              " 'indonesia': 578,\n",
              " 'instamood': 579,\n",
              " 'wanna': 580,\n",
              " 'skype': 581,\n",
              " 'may': 582,\n",
              " 'nice': 583,\n",
              " 'friendli': 584,\n",
              " 'pretend': 585,\n",
              " 'film': 586,\n",
              " 'congratul': 587,\n",
              " 'winner': 588,\n",
              " 'cheesydelight': 589,\n",
              " 'contest': 590,\n",
              " 'address': 591,\n",
              " 'guy': 592,\n",
              " 'market': 593,\n",
              " '24/7': 594,\n",
              " '14': 595,\n",
              " 'hour': 596,\n",
              " 'leav': 597,\n",
              " 'without': 598,\n",
              " 'delay': 599,\n",
              " 'actual': 600,\n",
              " 'easi': 601,\n",
              " 'guess': 602,\n",
              " 'train': 603,\n",
              " 'wd': 604,\n",
              " 'shift': 605,\n",
              " 'engin': 606,\n",
              " 'etc': 607,\n",
              " 'sunburn': 608,\n",
              " 'peel': 609,\n",
              " 'blog': 610,\n",
              " 'huge': 611,\n",
              " 'warm': 612,\n",
              " '‚òÜ': 613,\n",
              " 'complet': 614,\n",
              " 'triangl': 615,\n",
              " 'northern': 616,\n",
              " 'ireland': 617,\n",
              " 'sight': 618,\n",
              " 'smthng': 619,\n",
              " 'fr': 620,\n",
              " 'hug': 621,\n",
              " 'xoxo': 622,\n",
              " 'uu': 623,\n",
              " 'jaann': 624,\n",
              " 'topnewfollow': 625,\n",
              " 'connect': 626,\n",
              " 'wonder': 627,\n",
              " 'made': 628,\n",
              " 'fluffi': 629,\n",
              " 'insid': 630,\n",
              " 'pirouett': 631,\n",
              " 'moos': 632,\n",
              " 'trip': 633,\n",
              " 'philli': 634,\n",
              " 'decemb': 635,\n",
              " \"i'd\": 636,\n",
              " 'dude': 637,\n",
              " 'x41': 638,\n",
              " 'question': 639,\n",
              " 'flaw': 640,\n",
              " 'pain': 641,\n",
              " 'negat': 642,\n",
              " 'strength': 643,\n",
              " 'went': 644,\n",
              " 'solo': 645,\n",
              " 'move': 646,\n",
              " 'fav': 647,\n",
              " 'nirvana': 648,\n",
              " 'smell': 649,\n",
              " 'teen': 650,\n",
              " 'spirit': 651,\n",
              " 'rip': 652,\n",
              " 'ami': 653,\n",
              " 'winehous': 654,\n",
              " 'coupl': 655,\n",
              " 'tomhiddleston': 656,\n",
              " 'elizabetholsen': 657,\n",
              " 'yaytheylookgreat': 658,\n",
              " 'goodnight': 659,\n",
              " 'vid': 660,\n",
              " 'wake': 661,\n",
              " 'gonna': 662,\n",
              " 'shoot': 663,\n",
              " 'itti': 664,\n",
              " 'bitti': 665,\n",
              " 'teeni': 666,\n",
              " 'bikini': 667,\n",
              " 'much': 668,\n",
              " '4th': 669,\n",
              " 'togeth': 670,\n",
              " 'end': 671,\n",
              " 'xfile': 672,\n",
              " 'content': 673,\n",
              " 'rain': 674,\n",
              " 'fabul': 675,\n",
              " 'fantast': 676,\n",
              " '‚ô°': 677,\n",
              " 'jb': 678,\n",
              " 'forev': 679,\n",
              " 'belieb': 680,\n",
              " 'nighti': 681,\n",
              " 'bug': 682,\n",
              " 'bite': 683,\n",
              " 'bracelet': 684,\n",
              " 'idea': 685,\n",
              " 'foundri': 686,\n",
              " 'game': 687,\n",
              " 'sens': 688,\n",
              " 'pic': 689,\n",
              " 'ef': 690,\n",
              " 'phone': 691,\n",
              " 'woot': 692,\n",
              " 'derek': 693,\n",
              " 'use': 694,\n",
              " 'parkshar': 695,\n",
              " 'gloucestershir': 696,\n",
              " 'aaaahhh': 697,\n",
              " 'man': 698,\n",
              " 'traffic': 699,\n",
              " 'stress': 700,\n",
              " 'reliev': 701,\n",
              " \"how'r\": 702,\n",
              " 'arbeloa': 703,\n",
              " 'turn': 704,\n",
              " '17': 705,\n",
              " 'omg': 706,\n",
              " 'say': 707,\n",
              " 'europ': 708,\n",
              " 'rise': 709,\n",
              " 'find': 710,\n",
              " 'hard': 711,\n",
              " 'believ': 712,\n",
              " 'uncount': 713,\n",
              " 'coz': 714,\n",
              " 'unlimit': 715,\n",
              " 'cours': 716,\n",
              " 'teamposit': 717,\n",
              " 'aldub': 718,\n",
              " '‚òï': 719,\n",
              " 'rita': 720,\n",
              " 'info': 721,\n",
              " \"we'd\": 722,\n",
              " 'way': 723,\n",
              " 'boy': 724,\n",
              " 'x40': 725,\n",
              " 'true': 726,\n",
              " 'sethi': 727,\n",
              " 'high': 728,\n",
              " 'exe': 729,\n",
              " 'skeem': 730,\n",
              " 'saam': 731,\n",
              " 'peopl': 732,\n",
              " 'polit': 733,\n",
              " 'izzat': 734,\n",
              " 'wese': 735,\n",
              " 'trust': 736,\n",
              " 'khawateen': 737,\n",
              " 'k': 738,\n",
              " 'sath': 739,\n",
              " 'mana': 740,\n",
              " 'kar': 741,\n",
              " 'deya': 742,\n",
              " 'sort': 743,\n",
              " 'smart': 744,\n",
              " 'hair': 745,\n",
              " 'tbh': 746,\n",
              " 'jacob': 747,\n",
              " 'g': 748,\n",
              " 'upgrad': 749,\n",
              " 'tee': 750,\n",
              " 'famili': 751,\n",
              " 'person': 752,\n",
              " 'two': 753,\n",
              " 'convers': 754,\n",
              " 'onlin': 755,\n",
              " 'mclaren': 756,\n",
              " 'fridayfeel': 757,\n",
              " 'tgif': 758,\n",
              " 'squar': 759,\n",
              " 'enix': 760,\n",
              " 'bissmillah': 761,\n",
              " 'ya': 762,\n",
              " 'allah': 763,\n",
              " \"we'r\": 764,\n",
              " 'socent': 765,\n",
              " 'startup': 766,\n",
              " 'drop': 767,\n",
              " 'your': 768,\n",
              " 'arnd': 769,\n",
              " 'town': 770,\n",
              " 'basic': 771,\n",
              " 'piss': 772,\n",
              " 'cup': 773,\n",
              " 'also': 774,\n",
              " 'terribl': 775,\n",
              " 'complic': 776,\n",
              " 'discuss': 777,\n",
              " 'snapchat': 778,\n",
              " 'lynettelow': 779,\n",
              " 'kikmenow': 780,\n",
              " 'snapm': 781,\n",
              " 'hot': 782,\n",
              " 'amazon': 783,\n",
              " 'kikmeguy': 784,\n",
              " 'defin': 785,\n",
              " 'grow': 786,\n",
              " 'sport': 787,\n",
              " 'rt': 788,\n",
              " 'rakyat': 789,\n",
              " 'write': 790,\n",
              " 'sinc': 791,\n",
              " 'mention': 792,\n",
              " 'fli': 793,\n",
              " 'fish': 794,\n",
              " 'promot': 795,\n",
              " 'post': 796,\n",
              " 'cyber': 797,\n",
              " 'ourdaughtersourprid': 798,\n",
              " 'mypapamyprid': 799,\n",
              " 'papa': 800,\n",
              " 'coach': 801,\n",
              " 'posit': 802,\n",
              " 'kha': 803,\n",
              " 'atleast': 804,\n",
              " 'x39': 805,\n",
              " 'mango': 806,\n",
              " \"lassi'\": 807,\n",
              " \"monty'\": 808,\n",
              " 'marvel': 809,\n",
              " 'though': 810,\n",
              " 'suspect': 811,\n",
              " 'meant': 812,\n",
              " '24': 813,\n",
              " 'hr': 814,\n",
              " 'touch': 815,\n",
              " 'kepler': 816,\n",
              " '452b': 817,\n",
              " 'chalna': 818,\n",
              " 'hai': 819,\n",
              " 'thankyou': 820,\n",
              " 'hazel': 821,\n",
              " 'food': 822,\n",
              " 'brooklyn': 823,\n",
              " 'pta': 824,\n",
              " 'awak': 825,\n",
              " 'okayi': 826,\n",
              " 'awww': 827,\n",
              " 'ha': 828,\n",
              " 'doc': 829,\n",
              " 'splendid': 830,\n",
              " 'spam': 831,\n",
              " 'folder': 832,\n",
              " 'amount': 833,\n",
              " 'nigeria': 834,\n",
              " 'claim': 835,\n",
              " 'rted': 836,\n",
              " 'leg': 837,\n",
              " 'hurt': 838,\n",
              " 'bad': 839,\n",
              " 'mine': 840,\n",
              " 'saturday': 841,\n",
              " 'thaaank': 842,\n",
              " 'puhon': 843,\n",
              " 'happinesss': 844,\n",
              " 'tnc': 845,\n",
              " 'prior': 846,\n",
              " 'notif': 847,\n",
              " 'fat': 848,\n",
              " 'co': 849,\n",
              " 'probabl': 850,\n",
              " 'ate': 851,\n",
              " 'yuna': 852,\n",
              " 'tamesid': 853,\n",
              " '¬¥': 854,\n",
              " 'googl': 855,\n",
              " 'account': 856,\n",
              " 'scouser': 857,\n",
              " 'everyth': 858,\n",
              " 'zoe': 859,\n",
              " 'mate': 860,\n",
              " 'liter': 861,\n",
              " \"they'r\": 862,\n",
              " 'samee': 863,\n",
              " 'edgar': 864,\n",
              " 'updat': 865,\n",
              " 'log': 866,\n",
              " 'bring': 867,\n",
              " 'abe': 868,\n",
              " 'meet': 869,\n",
              " 'x38': 870,\n",
              " 'sigh': 871,\n",
              " 'dreamili': 872,\n",
              " 'pout': 873,\n",
              " 'eye': 874,\n",
              " 'quacketyquack': 875,\n",
              " 'funni': 876,\n",
              " 'happen': 877,\n",
              " 'phil': 878,\n",
              " 'em': 879,\n",
              " 'del': 880,\n",
              " 'rodder': 881,\n",
              " 'els': 882,\n",
              " 'play': 883,\n",
              " 'newest': 884,\n",
              " 'gamejam': 885,\n",
              " 'irish': 886,\n",
              " 'literatur': 887,\n",
              " 'inaccess': 888,\n",
              " \"kareena'\": 889,\n",
              " 'fan': 890,\n",
              " 'brain': 891,\n",
              " 'dot': 892,\n",
              " 'braindot': 893,\n",
              " 'fair': 894,\n",
              " 'rush': 895,\n",
              " 'either': 896,\n",
              " 'brandi': 897,\n",
              " '18': 898,\n",
              " 'carniv': 899,\n",
              " 'men': 900,\n",
              " 'put': 901,\n",
              " 'mask': 902,\n",
              " 'xavier': 903,\n",
              " 'forneret': 904,\n",
              " 'jennif': 905,\n",
              " 'site': 906,\n",
              " 'free': 907,\n",
              " '50.000': 908,\n",
              " '8': 909,\n",
              " 'ball': 910,\n",
              " 'pool': 911,\n",
              " 'coin': 912,\n",
              " 'edit': 913,\n",
              " 'trish': 914,\n",
              " '‚ô•': 915,\n",
              " 'grate': 916,\n",
              " 'three': 917,\n",
              " 'comment': 918,\n",
              " 'wakeup': 919,\n",
              " 'besid': 920,\n",
              " 'dirti': 921,\n",
              " 'sex': 922,\n",
              " 'lmaooo': 923,\n",
              " 'üò§': 924,\n",
              " 'loui': 925,\n",
              " \"he'\": 926,\n",
              " 'throw': 927,\n",
              " 'caus': 928,\n",
              " 'inspir': 929,\n",
              " 'ff': 930,\n",
              " 'twoof': 931,\n",
              " 'gr8': 932,\n",
              " 'wkend': 933,\n",
              " 'kind': 934,\n",
              " 'exhaust': 935,\n",
              " 'word': 936,\n",
              " 'cheltenham': 937,\n",
              " 'area': 938,\n",
              " 'kale': 939,\n",
              " 'crisp': 940,\n",
              " 'ruin': 941,\n",
              " 'x37': 942,\n",
              " 'open': 943,\n",
              " 'worldwid': 944,\n",
              " 'outta': 945,\n",
              " 'sfvbeta': 946,\n",
              " 'vantast': 947,\n",
              " 'xcylin': 948,\n",
              " 'bundl': 949,\n",
              " 'show': 950,\n",
              " 'internet': 951,\n",
              " 'price': 952,\n",
              " 'realisticli': 953,\n",
              " 'pay': 954,\n",
              " 'net': 955,\n",
              " 'educ': 956,\n",
              " 'power': 957,\n",
              " 'weapon': 958,\n",
              " 'nelson': 959,\n",
              " 'mandela': 960,\n",
              " 'recent': 961,\n",
              " 'j': 962,\n",
              " 'chenab': 963,\n",
              " 'flow': 964,\n",
              " 'pakistan': 965,\n",
              " 'incredibleindia': 966,\n",
              " 'teenchoic': 967,\n",
              " 'choiceinternationalartist': 968,\n",
              " 'superjunior': 969,\n",
              " 'caught': 970,\n",
              " 'first': 971,\n",
              " 'salmon': 972,\n",
              " 'super-blend': 973,\n",
              " 'project': 974,\n",
              " 'youth@bipolaruk.org.uk': 975,\n",
              " 'awesom': 976,\n",
              " 'stream': 977,\n",
              " 'alma': 978,\n",
              " 'mater': 979,\n",
              " 'highschoolday': 980,\n",
              " 'clientvisit': 981,\n",
              " 'faith': 982,\n",
              " 'christian': 983,\n",
              " 'school': 984,\n",
              " 'lizaminnelli': 985,\n",
              " 'upcom': 986,\n",
              " 'uk': 987,\n",
              " 'üòÑ': 988,\n",
              " 'singl': 989,\n",
              " 'hill': 990,\n",
              " 'everi': 991,\n",
              " 'beat': 992,\n",
              " 'wrong': 993,\n",
              " 'readi': 994,\n",
              " 'natur': 995,\n",
              " 'pefumeri': 996,\n",
              " 'workshop': 997,\n",
              " 'neal': 998,\n",
              " 'yard': 999,\n",
              " ...}"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7F3azBScrb5n"
      },
      "source": [
        "# **4. Converting tweet to a tensor**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDLff1V_W5X8",
        "outputId": "ad787e1e-6e46-470b-edb2-652ac2ec6961",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154
        }
      },
      "source": [
        "def tweet_to_tensor(tweet, vocab_dict, unk_token='__UNK__', verbose=False):\n",
        "  #process tweet into a list of words\n",
        "  word_l = process_tweet(tweet)\n",
        "\n",
        "  if verbose:\n",
        "    print('List of words from the processed tweet:', word_l)\n",
        "\n",
        "    #Initialize the list that will contain the unique integer ids for each word\n",
        "  tensor_l = []\n",
        "\n",
        "    #index of __UNK__\n",
        "  unk_id = vocab_dict[unk_token]\n",
        "\n",
        "    #if the word is in vocab then get its index else assign unk_id\n",
        "  for word in word_l:\n",
        "    word_id = vocab_dict[word] if word in vocab_dict else unk_id\n",
        "\n",
        "    tensor_l.append(word_id)\n",
        "\n",
        "  return tensor_l\n",
        "\n",
        "\n",
        "print('Original tweet in validation set:', val_pos[0], '\\n')\n",
        "print('\\ntensor of tweet:', tweet_to_tensor(val_pos[0], vocab_dict = vocab, unk_token='__UNK__', verbose = True))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original tweet in validation set: Bro:U wan cut hair anot,ur hair long Liao bo\n",
            "Me:since ord liao,take it easy lor treat as save $ leave it longer :)\n",
            "Bro:LOL Sibei xialan \n",
            "\n",
            "List of words from the processed tweet: ['bro', 'u', 'wan', 'cut', 'hair', 'anot', 'ur', 'hair', 'long', 'liao', 'bo', 'sinc', 'ord', 'liao', 'take', 'easi', 'lor', 'treat', 'save', 'leav', 'longer', ':)', 'bro', 'lol', 'sibei', 'xialan']\n",
            "\n",
            "tensor of tweet: [1065, 136, 479, 2351, 745, 8146, 1123, 745, 53, 2, 2672, 791, 2, 2, 349, 601, 2, 3489, 1017, 597, 4559, 9, 1065, 157, 2, 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F45JyTLorTug"
      },
      "source": [
        "# **5. Creating batch generator**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xd5UDpiCelnp"
      },
      "source": [
        "def data_generator(data_pos, data_neg, batch_size, loop, vocab_dict, shuffle=False):\n",
        "  \n",
        "  assert batch_size % 2 == 0\n",
        "\n",
        "  n_to_take = batch_size // 2\n",
        "\n",
        "  pos_index = 0\n",
        "  neg_index = 0\n",
        "\n",
        "  len_data_pos = len(data_pos)\n",
        "  len_data_neg = len(data_neg)\n",
        "\n",
        "  pos_index_lines = list(range(len_data_pos))\n",
        "  neg_index_lines = list(range(len_data_neg))\n",
        "\n",
        "  if shuffle:\n",
        "    rnd.shuffle(pos_index_lines)\n",
        "    rnd.shuffle(neg_index_lines)\n",
        "  \n",
        "  stop = False\n",
        "\n",
        "  while not stop:\n",
        "\n",
        "    #create a batch with pos and neg examples\n",
        "    batch = []\n",
        "\n",
        "    # positive examples\n",
        "    for i in range(n_to_take):\n",
        "      if pos_index >= len_data_pos:\n",
        "        if not loop:\n",
        "          stop = True;\n",
        "          break;\n",
        "        \n",
        "        pos_index = 0\n",
        "\n",
        "        if shuffle:\n",
        "          #shuffle the index of the positive sample\n",
        "          rnd.shuffle(pos_index_lines)\n",
        "\n",
        "      #get the tweet as positive index\n",
        "      tweet = data_pos[pos_index_lines[pos_index]]\n",
        "\n",
        "      #convert the tweet into tensor of integers\n",
        "      tensor = tweet_to_tensor(tweet, vocab_dict)\n",
        "\n",
        "      #append the tensor to the batch list\n",
        "      batch.append(tensor)\n",
        "\n",
        "      pos_index = pos_index + 1\n",
        "\n",
        "    # Negative examples\n",
        "    for i in range(n_to_take):\n",
        "      if neg_index > len_data_neg:\n",
        "        if not loop:\n",
        "          stop = True\n",
        "          break;\n",
        "\n",
        "        neg_index = 0\n",
        "\n",
        "        if shuffle:\n",
        "          #shuffle the index of the negative sample\n",
        "          rnd.shuffle(neg_index_lines)\n",
        "\n",
        "      # Get the tweet as negative index\n",
        "      tweet = data_neg[neg_index_lines[neg_index]]\n",
        "\n",
        "      #convert the tweet into tensor of integers\n",
        "      tensor = tweet_to_tensor(tweet, vocab_dict)\n",
        "\n",
        "      #append the tensor to the batch list\n",
        "      batch.append(tensor)\n",
        "\n",
        "      neg_index = neg_index + 1\n",
        "\n",
        "    if stop:\n",
        "      break;\n",
        "\n",
        "    # update the start index for positive data so that its n_to_take positions after the current pos_index\n",
        "    pos_index += n_to_take\n",
        "\n",
        "    # update the start index for negative data so that its n_to_take positions after the current neg_index\n",
        "    neg_index += n_to_take\n",
        "\n",
        "    #Get the max tweet length\n",
        "    max_len = max([len(t) for t in batch])\n",
        "\n",
        "    #Padded version of the tensors\n",
        "    tensor_pad_l = []\n",
        "\n",
        "    for tensor in batch:\n",
        "      #get the number of positions to pad from each tensor to make it to max_len long\n",
        "      n_pad = max_len - len(tensor)\n",
        "\n",
        "      pad_l = [0]*n_pad\n",
        "\n",
        "      tensor_pad = tensor + pad_l\n",
        "\n",
        "      tensor_pad_l.append(tensor_pad)\n",
        "\n",
        "    #convert the list of padded tensors to numpy array\n",
        "    inputs = np.array(tensor_pad_l)\n",
        "\n",
        "    #generate the list of targets for the positive examples \n",
        "    target_pos = [1]*n_to_take\n",
        "\n",
        "    #generate the list of targets for the negative examples\n",
        "    target_neg = [0]*n_to_take\n",
        "\n",
        "    target_l = target_pos + target_neg\n",
        "\n",
        "    #convert the target list to array\n",
        "    targets = np.array(target_l)\n",
        "\n",
        "    #Treat all examples equally important\n",
        "    example_weights = np.ones_like(targets)\n",
        "\n",
        "    yield inputs, targets, example_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wk4S4zDaelp-",
        "outputId": "45482bcc-24fd-4c0a-b834-af046948182c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        }
      },
      "source": [
        "rnd.seed(30)\n",
        "\n",
        "#Create training data generator\n",
        "def train_generator(batch_size, shuffle = False):\n",
        "  return data_generator(train_pos, train_neg, batch_size, True, vocab, shuffle)\n",
        "\n",
        "#Create validation data generator\n",
        "def validation_generator(batch_size, shuffle = False):\n",
        "  return data_generator(val_pos, val_neg, batch_size, True, vocab, shuffle)\n",
        "\n",
        "def test_generator(batch_size, shuffle = False):\n",
        "  return data_generator(val_pos, val_neg, batch_size, False, vocab, shuffle)\n",
        "\n",
        "inputs, targets, example_weights = next(train_generator(4, shuffle=True))\n",
        "\n",
        "print('Input tensors:', inputs, '\\n')\n",
        "print('targets:', targets, '\\n')\n",
        "print('Weights:', example_weights, '\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input tensors: [[2005 4451 3201    9    0    0    0    0    0    0    0]\n",
            " [4954  567 2000 1454 5174 3499  141 3499  130  459    9]\n",
            " [3761  109  136  583 2930 3969    0    0    0    0    0]\n",
            " [ 250 3761    0    0    0    0    0    0    0    0    0]] \n",
            "\n",
            "targets: [1 1 0 0] \n",
            "\n",
            "Weights: [1 1 1 1] \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J72sHWuVelsg",
        "outputId": "604a5fcf-afcd-4456-e492-3366705fe18c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "source": [
        "#Create a data generator for training data\n",
        "tmp_data_gen = train_generator(batch_size = 4)\n",
        "\n",
        "tmp_inputs, tmp_targets, tmp_example_weights = next(tmp_data_gen)\n",
        "\n",
        "print('Input shape:', tmp_inputs.shape)\n",
        "print('target shape:', tmp_targets.shape)\n",
        "print('Weights shape:', tmp_example_weights.shape, '\\n')\n",
        "\n",
        "for i, t in enumerate(tmp_inputs):\n",
        "  print(f'Input tensor:{t}; target:{tmp_targets[i]}; weights:{tmp_example_weights[i]}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input shape: (4, 14)\n",
            "target shape: (4,)\n",
            "Weights shape: (4,) \n",
            "\n",
            "Input tensor:[3 4 5 6 7 8 9 0 0 0 0 0 0 0]; target:1; weights:1\n",
            "Input tensor:[10 11 12 13 14 15 16 17 18 19 20  9 21 22]; target:1; weights:1\n",
            "Input tensor:[5738 2901 3761    0    0    0    0    0    0    0    0    0    0    0]; target:0; weights:1\n",
            "Input tensor:[ 858  256 3652 5739  307 4458  567 1230 2767  328 1202 3761    0    0]; target:0; weights:1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2Z8HRaEW5aC"
      },
      "source": [
        "# use the fastmath module within trax\n",
        "from trax import fastmath\n",
        "\n",
        "# use the numpy module from trax\n",
        "np = fastmath.numpy\n",
        "\n",
        "# use the fastmath.random module from trax\n",
        "random = fastmath.random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4ZlNMvzsb7R"
      },
      "source": [
        "# **6. Creating a model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_wQD2wC059L",
        "outputId": "b7d3fbc3-b938-4298-f84e-a328e5da4a8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "def classifier(vocab_size = len(vocab), embedding_dim = 256, output_dim = 2, mode = 'train'):\n",
        "  #create embedding layer\n",
        "  embed_layer = tl.Embedding(vocab_size=vocab_size, d_feature=embedding_dim)\n",
        "  #Create a mean layer to create an average word embedding\n",
        "  mean_layer = tl.Mean(axis=1)\n",
        "  #create a dense layer, one unit for each output\n",
        "  dense_layer = tl.Dense(n_units = output_dim)\n",
        "  #create log softmax layer\n",
        "  log_softmax_layer = tl.LogSoftmax()\n",
        "\n",
        "  model = tl.Serial(\n",
        "      embed_layer,\n",
        "      mean_layer,\n",
        "      dense_layer,\n",
        "      log_softmax_layer\n",
        "  )\n",
        "\n",
        "  return model\n",
        "\n",
        "tmp_model = classifier()\n",
        "print(type(tmp_model))\n",
        "display(tmp_model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'trax.layers.combinators.Serial'>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Serial[\n",
              "  Embedding_9092_256\n",
              "  Mean\n",
              "  Dense_2\n",
              "  LogSoftmax\n",
              "]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6yqWFgkrAx5"
      },
      "source": [
        "# **7. Training the model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mslxDAkB3uDI"
      },
      "source": [
        "from trax.supervised import training\n",
        "\n",
        "batch_size = 16\n",
        "rnd.seed(271)\n",
        "\n",
        "train_task = training.TrainTask(\n",
        "    labeled_data = train_generator(batch_size = batch_size, shuffle=True),\n",
        "    loss_layer = tl.CrossEntropyLoss(),\n",
        "    optimizer = trax.optimizers.Adam(0.01),\n",
        "    n_steps_per_checkpoint = 10,\n",
        ")\n",
        "\n",
        "eval_task = training.EvalTask(\n",
        "    labeled_data = validation_generator(batch_size=batch_size, shuffle=True),\n",
        "    metrics = [tl.CrossEntropyLoss(), tl.Accuracy()],\n",
        ")\n",
        "\n",
        "model = classifier()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjimL3Jn52EF",
        "outputId": "12d6c1f4-cd41-4028-faab-4047c023437c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "output_dir = 'model/'\n",
        "output_dir_expand = os.path.expanduser(output_dir)\n",
        "print(output_dir_expand)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/root/model/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6NZ57Dv52Hj"
      },
      "source": [
        "def train_model(classifier, train_task, eval_task, n_steps, output_dir):\n",
        "  training_loop = training.Loop(\n",
        "      classifier,\n",
        "      train_task,\n",
        "      eval_task = eval_task,\n",
        "      output_dir=output_dir\n",
        "  )\n",
        "  training_loop.run(n_steps = n_steps)\n",
        "  return training_loop"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0F0Wl5LP3uGp",
        "outputId": "9d086739-99c6-450e-d2ef-8429f096670a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 571
        }
      },
      "source": [
        "training_loop = train_model(model, train_task, eval_task, 100, output_dir_expand)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step      1: train CrossEntropyLoss |  0.71663344\n",
            "Step      1: eval  CrossEntropyLoss |  0.80896854\n",
            "Step      1: eval          Accuracy |  0.56250000\n",
            "Step     10: train CrossEntropyLoss |  0.65261537\n",
            "Step     10: eval  CrossEntropyLoss |  0.67353255\n",
            "Step     10: eval          Accuracy |  0.43750000\n",
            "Step     20: train CrossEntropyLoss |  0.41853198\n",
            "Step     20: eval  CrossEntropyLoss |  0.32247627\n",
            "Step     20: eval          Accuracy |  0.93750000\n",
            "Step     30: train CrossEntropyLoss |  0.30088764\n",
            "Step     30: eval  CrossEntropyLoss |  0.27893353\n",
            "Step     30: eval          Accuracy |  1.00000000\n",
            "Step     40: train CrossEntropyLoss |  0.17787032\n",
            "Step     40: eval  CrossEntropyLoss |  0.18334462\n",
            "Step     40: eval          Accuracy |  0.93750000\n",
            "Step     50: train CrossEntropyLoss |  0.15455118\n",
            "Step     50: eval  CrossEntropyLoss |  0.05755305\n",
            "Step     50: eval          Accuracy |  1.00000000\n",
            "Step     60: train CrossEntropyLoss |  0.12856857\n",
            "Step     60: eval  CrossEntropyLoss |  0.15309972\n",
            "Step     60: eval          Accuracy |  0.93750000\n",
            "Step     70: train CrossEntropyLoss |  0.09916756\n",
            "Step     70: eval  CrossEntropyLoss |  0.05575932\n",
            "Step     70: eval          Accuracy |  1.00000000\n",
            "Step     80: train CrossEntropyLoss |  0.05752111\n",
            "Step     80: eval  CrossEntropyLoss |  0.11135875\n",
            "Step     80: eval          Accuracy |  0.93750000\n",
            "Step     90: train CrossEntropyLoss |  0.09764536\n",
            "Step     90: eval  CrossEntropyLoss |  0.03030130\n",
            "Step     90: eval          Accuracy |  1.00000000\n",
            "Step    100: train CrossEntropyLoss |  0.04964162\n",
            "Step    100: eval  CrossEntropyLoss |  0.11302563\n",
            "Step    100: eval          Accuracy |  1.00000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mZIFFb1q2tx"
      },
      "source": [
        "# **8. Making a prediction on sample training data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSz6Gy3pW5dl",
        "outputId": "4612227d-97b6-4208-9349-a2997ed194cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "#prediction\n",
        "\n",
        "# Create a generator object\n",
        "tmp_train_generator = train_generator(16)\n",
        "\n",
        "# get one batch\n",
        "tmp_batch = next(tmp_train_generator)\n",
        "\n",
        "tmp_inputs, tmp_targets, tmp_example_weights = tmp_batch\n",
        "print(tmp_inputs.shape)\n",
        "print(tmp_targets.shape)\n",
        "print(tmp_example_weights)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(16, 15)\n",
            "(16,)\n",
            "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0j3Bnp4J9wRT",
        "outputId": "834211e5-05d4-4dca-f8e8-3499b5bc823c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        }
      },
      "source": [
        "# feed the tweet tensors into the model to get a prediction\n",
        "tmp_pred = training_loop.eval_model(tmp_inputs)\n",
        "\n",
        "print(f\"The prediction shape is {tmp_pred.shape}, num of tensor_tweets as rows\")\n",
        "print(\"Column 0 is the probability of a negative sentiment (class 0)\")\n",
        "print(\"Column 1 is the probability of a positive sentiment (class 1)\")\n",
        "print(\"View the prediction array\")\n",
        "tmp_pred"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The prediction shape is (16, 2), num of tensor_tweets as rows\n",
            "Column 0 is the probability of a negative sentiment (class 0)\n",
            "Column 1 is the probability of a positive sentiment (class 1)\n",
            "View the prediction array\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeviceArray([[-3.7626619e+00, -2.3495674e-02],\n",
              "             [-4.1175117e+00, -1.6419172e-02],\n",
              "             [-4.3073044e+00, -1.3561487e-02],\n",
              "             [-2.6957486e+00, -6.9877386e-02],\n",
              "             [-4.0748644e+00, -1.7140627e-02],\n",
              "             [-2.1877446e+00, -1.1897421e-01],\n",
              "             [-6.6753860e+00, -1.2621880e-03],\n",
              "             [-3.5692210e+00, -2.8582335e-02],\n",
              "             [-2.3403168e-03, -6.0587635e+00],\n",
              "             [-1.1348724e-03, -6.7817626e+00],\n",
              "             [-8.5546970e-03, -4.7655659e+00],\n",
              "             [-8.5353851e-05, -9.3677816e+00],\n",
              "             [-2.8588772e-03, -5.8587508e+00],\n",
              "             [-1.0375977e-03, -6.8714089e+00],\n",
              "             [-2.4724007e-03, -6.0037174e+00],\n",
              "             [-2.2265911e-03, -6.1084499e+00]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-OnJqu9G9wUI",
        "outputId": "782a0557-6bfb-4e15-e748-d28583137597",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "#turn probabilities into category predictions\n",
        "tmp_is_positive = tmp_pred[:,1] > tmp_pred[:,0]\n",
        "for i,p in enumerate(tmp_is_positive):\n",
        "   print(f\"Neg log prob {tmp_pred[i,0]:.4f}\\tPos log prob {tmp_pred[i,1]:.4f}\\t is positive? {p}\\t actual {tmp_targets[i]}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Neg log prob -3.7627\tPos log prob -0.0235\t is positive? True\t actual 1\n",
            "Neg log prob -4.1175\tPos log prob -0.0164\t is positive? True\t actual 1\n",
            "Neg log prob -4.3073\tPos log prob -0.0136\t is positive? True\t actual 1\n",
            "Neg log prob -2.6957\tPos log prob -0.0699\t is positive? True\t actual 1\n",
            "Neg log prob -4.0749\tPos log prob -0.0171\t is positive? True\t actual 1\n",
            "Neg log prob -2.1877\tPos log prob -0.1190\t is positive? True\t actual 1\n",
            "Neg log prob -6.6754\tPos log prob -0.0013\t is positive? True\t actual 1\n",
            "Neg log prob -3.5692\tPos log prob -0.0286\t is positive? True\t actual 1\n",
            "Neg log prob -0.0023\tPos log prob -6.0588\t is positive? False\t actual 0\n",
            "Neg log prob -0.0011\tPos log prob -6.7818\t is positive? False\t actual 0\n",
            "Neg log prob -0.0086\tPos log prob -4.7656\t is positive? False\t actual 0\n",
            "Neg log prob -0.0001\tPos log prob -9.3678\t is positive? False\t actual 0\n",
            "Neg log prob -0.0029\tPos log prob -5.8588\t is positive? False\t actual 0\n",
            "Neg log prob -0.0010\tPos log prob -6.8714\t is positive? False\t actual 0\n",
            "Neg log prob -0.0025\tPos log prob -6.0037\t is positive? False\t actual 0\n",
            "Neg log prob -0.0022\tPos log prob -6.1084\t is positive? False\t actual 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWuuTxW2qrVO"
      },
      "source": [
        "# **9. Computing accuracy on the batch**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CnII4Sxm9wWX"
      },
      "source": [
        "def computing_accuracy(preds, y, y_weights):\n",
        "  is_pos = preds[:,1] > preds[:,0]\n",
        "  \n",
        "  # convert the array of booleans into an array of np.int32\n",
        "  is_pos_int = is_pos.astype(np.int32)\n",
        "  \n",
        "  #compare predictions with actual\n",
        "  correct = is_pos_int == y\n",
        "  \n",
        "  sum_weights = np.sum(y_weights)\n",
        "  \n",
        "  # convert the array of correct predictions (boolean) into an arrayof np.float32\n",
        "  correct_float = correct.astype(np.float32)\n",
        "  \n",
        "  # Multiply each prediction with its corresponding weight.\n",
        "  weighted_correct_float = correct_float * y_weights\n",
        "\n",
        "  # Sum up the weighted correct predictions (of type np.float32), to go in the denominator.\n",
        "  weighted_num_correct = np.sum(weighted_correct_float)\n",
        " \n",
        "  accuracy = weighted_num_correct / sum_weights\n",
        "\n",
        "  return accuracy, weighted_num_correct, sum_weights\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aq2Bx6-I9wZc",
        "outputId": "c57a9832-8ba4-49dd-db6b-e445edface3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "tmp_val_generator = validation_generator(128)\n",
        "tmp_batch = next(tmp_val_generator)\n",
        "tmp_inputs, tmp_targets, tmp_example_weights = tmp_batch\n",
        "tmp_pred = training_loop.eval_model(tmp_inputs)\n",
        "tmp_acc, tmp_num_correct, tmp_num_predictions = computing_accuracy(preds=tmp_pred, y=tmp_targets, y_weights=tmp_example_weights)\n",
        "\n",
        "print(f\"Model's prediction accuracy on a single training batch is: {100 * tmp_acc}%\")\n",
        "print(f\"Weighted number of correct predictions {tmp_num_correct}; weighted number of total observations predicted {tmp_num_predictions}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model's prediction accuracy on a single training batch is: 92.1875%\n",
            "Weighted number of correct predictions 118.0; weighted number of total observations predicted 128\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lV97Jx_hqg0N"
      },
      "source": [
        "# **10. Testing your model on validation data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uw6Bt4_TGUBa"
      },
      "source": [
        "def test_model(generator, model):\n",
        "    \n",
        "    accuracy = 0.\n",
        "    total_num_correct = 0\n",
        "    total_num_pred = 0\n",
        "    \n",
        "    ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
        "    for batch in generator: \n",
        "        \n",
        "        # Retrieve the inputs from the batch\n",
        "        inputs = batch[0]\n",
        "        \n",
        "        # Retrieve the targets (actual labels) from the batch\n",
        "        targets =  batch[1]\n",
        "        \n",
        "        # Retrieve the example weight.\n",
        "        example_weight = batch[2]\n",
        "\n",
        "        # Make predictions using the inputs\n",
        "        pred = model(inputs)\n",
        "        \n",
        "        # Calculate accuracy for the batch by comparing its predictions and targets\n",
        "        batch_accuracy, batch_num_correct, batch_num_pred = computing_accuracy(pred, targets, example_weight) \n",
        "        \n",
        "        # Update the total number of correct predictions\n",
        "        # by adding the number of correct predictions from this batch\n",
        "        total_num_correct += batch_num_correct\n",
        "        \n",
        "        # Update the total number of predictions \n",
        "        # by adding the number of predictions made for the batch\n",
        "        total_num_pred += batch_num_pred\n",
        "\n",
        "    # Calculate accuracy over all examples\n",
        "    accuracy = total_num_correct / total_num_pred\n",
        "    \n",
        "    ### END CODE HERE ###\n",
        "    return accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCrRuk_WGUa0",
        "outputId": "4d1c86f5-7d76-4ea6-f1ea-575440b502c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model = training_loop.eval_model\n",
        "accuracy = test_model(test_generator(16), model)\n",
        "\n",
        "print(f'The accuracy of your model on the validation set is {accuracy:.4f}', )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The accuracy of your model on the validation set is 0.9702\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDaSmLsIqAbm"
      },
      "source": [
        "# **11.Testing your own sentence**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gY1YdDAHoUL"
      },
      "source": [
        "def predict(sentence):\n",
        "  inputs = np.array(tweet_to_tensor(sentence, vocab_dict = vocab))\n",
        "  \n",
        "  # Batch size 1, add dimension for batch, to work with the model\n",
        "  inputs = inputs[None, :]  \n",
        "\n",
        "  # predict with the model\n",
        "  preds_probs = model(inputs)\n",
        "\n",
        "  # Turn probabilities into categories\n",
        "  preds = int(preds_probs[0, 1] > preds_probs[0, 0])\n",
        "\n",
        "  sentiment = \"negative\"\n",
        "  if preds == 1:\n",
        "    sentiment = 'positive'\n",
        "\n",
        "  return preds, sentiment  \n",
        "      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sreaL2UiILB7",
        "outputId": "bbda86ff-1aa6-4f56-9f1e-ce5e6649f866",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "#positive tweet\n",
        "sentence = \"its a wonderful day today\"\n",
        "tmp_pred, tmp_sentiment = predict(sentence)\n",
        "print(f\"The sentiment of the sentence \\n***\\n\\\"{sentence}\\\"\\n***\\nis {tmp_sentiment}.\")\n",
        "\n",
        "#negative tweet\n",
        "sentence = \"very disappointing\"\n",
        "tmp_pred, tmp_sentiment = predict(sentence)\n",
        "print(f\"The sentiment of the sentence \\n***\\n\\\"{sentence}\\\"\\n***\\nis {tmp_sentiment}.\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The sentiment of the sentence \n",
            "***\n",
            "\"its a wonderful day today\"\n",
            "***\n",
            "is positive.\n",
            "The sentiment of the sentence \n",
            "***\n",
            "\"very disappointing\"\n",
            "***\n",
            "is negative.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AuVD0Wj1ILFQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}